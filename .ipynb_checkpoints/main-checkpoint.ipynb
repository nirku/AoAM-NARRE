{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:07:25.682174Z",
     "start_time": "2020-07-15T16:07:20.842702Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loaddata.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T15:55:08.611740Z",
     "start_time": "2020-07-15T15:55:07.165201Z"
    }
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './data/kindle/Kindle_Store_5.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f4c7ab2ec25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mTP_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTPS_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Kindle_Store_5.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTP_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0musers_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mitems_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: './data/kindle/Kindle_Store_5.json'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data pre process\n",
    "\n",
    "@author:\n",
    "Chong Chen (cstchenc@163.com)\n",
    "\n",
    "@ created:\n",
    "25/8/2017\n",
    "@references:\n",
    "'''\n",
    "\n",
    "TPS_DIR = './data/kindle'\n",
    "TP_file = os.path.join(TPS_DIR, 'Kindle_Store_5.json')\n",
    "\n",
    "f= open(TP_file)\n",
    "users_id=[]\n",
    "items_id=[]\n",
    "ratings=[]\n",
    "reviews=[]\n",
    "np.random.seed(2017)\n",
    "\n",
    "for line in f:\n",
    "    js=json.loads(line)\n",
    "    if str(js['reviewerID'])=='unknown':\n",
    "        print \"unknown\"\n",
    "        continue\n",
    "    if str(js['asin'])=='unknown':\n",
    "        print \"unknown2\"\n",
    "        continue\n",
    "    if 'reviewText' not in js:\n",
    "        continue\n",
    "    reviews.append(js['reviewText'])\n",
    "    users_id.append(str(js['reviewerID'])+',')\n",
    "    items_id.append(str(js['asin'])+',')\n",
    "    ratings.append(str(js['overall']))\n",
    "data=pd.DataFrame({'user_id':pd.Series(users_id),\n",
    "                   'item_id':pd.Series(items_id),\n",
    "                   'ratings':pd.Series(ratings),\n",
    "                   'reviews':pd.Series(reviews)})[['user_id','item_id','ratings','reviews']]\n",
    "\n",
    "def get_count(tp, id):\n",
    "    playcount_groupbyid = tp[[id, 'ratings']].groupby(id, as_index=False)\n",
    "    count = playcount_groupbyid.size()\n",
    "    return count\n",
    "usercount, itemcount = get_count(data, 'user_id'), get_count(data, 'item_id')\n",
    "unique_uid = usercount.index\n",
    "unique_sid = itemcount.index\n",
    "item2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
    "user2id = dict((uid, i) for (i, uid) in enumerate(unique_uid))\n",
    "def numerize(tp):\n",
    "    uid = map(lambda x: user2id[x], tp['user_id'])\n",
    "    sid = map(lambda x: item2id[x], tp['item_id'])\n",
    "    tp['user_id'] = uid\n",
    "    tp['item_id'] = sid\n",
    "    return tp\n",
    "\n",
    "data=numerize(data)\n",
    "tp_rating=data[['user_id','item_id','ratings']]\n",
    "\n",
    "\n",
    "n_ratings = tp_rating.shape[0]\n",
    "test = np.random.choice(n_ratings, size=int(0.20 * n_ratings), replace=False)\n",
    "test_idx = np.zeros(n_ratings, dtype=bool)\n",
    "test_idx[test] = True\n",
    "\n",
    "tp_1 = tp_rating[test_idx]\n",
    "tp_train= tp_rating[~test_idx]\n",
    "\n",
    "data2=data[test_idx]\n",
    "data=data[~test_idx]\n",
    "\n",
    "\n",
    "n_ratings = tp_1.shape[0]\n",
    "test = np.random.choice(n_ratings, size=int(0.50 * n_ratings), replace=False)\n",
    "\n",
    "test_idx = np.zeros(n_ratings, dtype=bool)\n",
    "test_idx[test] = True\n",
    "\n",
    "tp_test = tp_1[test_idx]\n",
    "tp_valid = tp_1[~test_idx]\n",
    "tp_train.to_csv(os.path.join(TPS_DIR, 'kindle_train.csv'), index=False,header=None)\n",
    "tp_valid.to_csv(os.path.join(TPS_DIR, 'kindle_valid.csv'), index=False,header=None)\n",
    "tp_test.to_csv(os.path.join(TPS_DIR, 'kindle_test.csv'), index=False,header=None)\n",
    "\n",
    "user_reviews={}\n",
    "item_reviews={}\n",
    "user_rid={}\n",
    "item_rid={}\n",
    "for i in data.values:\n",
    "    if user_reviews.has_key(i[0]):\n",
    "        user_reviews[i[0]].append(i[3])\n",
    "        user_rid[i[0]].append(i[1])\n",
    "    else:\n",
    "        user_rid[i[0]]=[i[1]]\n",
    "        user_reviews[i[0]]=[i[3]]\n",
    "    if item_reviews.has_key(i[1]):\n",
    "        item_reviews[i[1]].append(i[3])\n",
    "        item_rid[i[1]].append(i[0])\n",
    "    else:\n",
    "        item_reviews[i[1]] = [i[3]]\n",
    "        item_rid[i[1]]=[i[0]]\n",
    "\n",
    "\n",
    "for i in data2.values:\n",
    "    if user_reviews.has_key(i[0]):\n",
    "        l=1\n",
    "    else:\n",
    "        user_rid[i[0]]=[0]\n",
    "        user_reviews[i[0]]=['0']\n",
    "    if item_reviews.has_key(i[1]):\n",
    "        l=1\n",
    "    else:\n",
    "        item_reviews[i[1]] = [0]\n",
    "        item_rid[i[1]]=['0']\n",
    "\n",
    "pickle.dump(user_reviews, open(os.path.join(TPS_DIR, 'user_review'), 'wb'))\n",
    "pickle.dump(item_reviews, open(os.path.join(TPS_DIR, 'item_review'), 'wb'))\n",
    "pickle.dump(user_rid, open(os.path.join(TPS_DIR, 'user_rid'), 'wb'))\n",
    "pickle.dump(item_rid, open(os.path.join(TPS_DIR, 'item_rid'), 'wb'))\n",
    "\n",
    "usercount, itemcount = get_count(data, 'user_id'), get_count(data, 'item_id')\n",
    "\n",
    "\n",
    "print np.sort(np.array(usercount.values))\n",
    "print np.sort(np.array(itemcount.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_pro.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T17:42:48.028954Z",
     "start_time": "2020-07-01T17:38:43.087694Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    try:\n",
    "        string = re.sub(r\"[^A-Za-z]\", \" \", string)\n",
    "        string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "        string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "        string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "        string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "        string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "        string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "        string = re.sub(r\",\", \" , \", string)\n",
    "        string = re.sub(r\"!\", \" ! \", string)\n",
    "        string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "        string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "        string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "        string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    except:\n",
    "        print string\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def pad_sentences(u_text, u_len, u2_len, padding_word=\"<PAD/>\"):\n",
    "    \"\"\"\n",
    "    Pads all sentences to the same length. The length is defined by the longest sentence.\n",
    "    Returns padded sentences.\n",
    "    \"\"\"\n",
    "    review_num = u_len\n",
    "    review_len = u2_len\n",
    "\n",
    "    u_text2 = {}\n",
    "    for i in u_text.keys():\n",
    "        u_reviews = u_text[i]\n",
    "        padded_u_train = []\n",
    "        for ri in range(review_num):\n",
    "            if ri < len(u_reviews):\n",
    "                sentence = u_reviews[ri]\n",
    "                if review_len > len(sentence):\n",
    "                    num_padding = review_len - len(sentence)\n",
    "                    new_sentence = sentence + [padding_word] * num_padding\n",
    "                    padded_u_train.append(new_sentence)\n",
    "                else:\n",
    "                    new_sentence = sentence[:review_len]\n",
    "                    padded_u_train.append(new_sentence)\n",
    "            else:\n",
    "                new_sentence = [padding_word] * review_len\n",
    "                padded_u_train.append(new_sentence)\n",
    "        u_text2[i] = padded_u_train\n",
    "\n",
    "    return u_text2\n",
    "\n",
    "\n",
    "def pad_reviewid(u_train, u_valid, u_len, num):\n",
    "    pad_u_train = []\n",
    "\n",
    "    for i in range(len(u_train)):\n",
    "        x = u_train[i]\n",
    "        while u_len > len(x):\n",
    "            x.append(num)\n",
    "        if u_len < len(x):\n",
    "            x = x[:u_len]\n",
    "        pad_u_train.append(x)\n",
    "    pad_u_valid = []\n",
    "\n",
    "    for i in range(len(u_valid)):\n",
    "        x = u_valid[i]\n",
    "        while u_len > len(x):\n",
    "            x.append(num)\n",
    "        if u_len < len(x):\n",
    "            x = x[:u_len]\n",
    "        pad_u_valid.append(x)\n",
    "    return pad_u_train, pad_u_valid\n",
    "\n",
    "\n",
    "def build_vocab(sentences1, sentences2):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary mapping from word to index based on the sentences.\n",
    "    Returns vocabulary mapping and inverse vocabulary mapping.\n",
    "    \"\"\"\n",
    "    # Build vocabulary\n",
    "    word_counts1 = Counter(itertools.chain(*sentences1))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv1 = [x[0] for x in word_counts1.most_common()]\n",
    "    vocabulary_inv1 = list(sorted(vocabulary_inv1))\n",
    "    # Mapping from word to index\n",
    "    vocabulary1 = {x: i for i, x in enumerate(vocabulary_inv1)}\n",
    "\n",
    "    word_counts2 = Counter(itertools.chain(*sentences2))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv2 = [x[0] for x in word_counts2.most_common()]\n",
    "    vocabulary_inv2 = list(sorted(vocabulary_inv2))\n",
    "    # Mapping from word to index\n",
    "    vocabulary2 = {x: i for i, x in enumerate(vocabulary_inv2)}\n",
    "    return [vocabulary1, vocabulary_inv1, vocabulary2, vocabulary_inv2]\n",
    "\n",
    "\n",
    "def build_input_data(u_text, i_text, vocabulary_u, vocabulary_i):\n",
    "    \"\"\"\n",
    "    Maps sentencs and labels to vectors based on a vocabulary.\n",
    "    \"\"\"\n",
    "    l = len(u_text)\n",
    "    u_text2 = {}\n",
    "    for i in u_text.keys():\n",
    "        u_reviews = u_text[i]\n",
    "        u = np.array([[vocabulary_u[word] for word in words] for words in u_reviews])\n",
    "        u_text2[i] = u\n",
    "    l = len(i_text)\n",
    "    i_text2 = {}\n",
    "    for j in i_text.keys():\n",
    "        i_reviews = i_text[j]\n",
    "        i = np.array([[vocabulary_i[word] for word in words] for words in i_reviews])\n",
    "        i_text2[j] = i\n",
    "    return u_text2, i_text2\n",
    "\n",
    "\n",
    "def load_data(train_data, valid_data, user_review, item_review, user_rid, item_rid, stopwords):\n",
    "    \"\"\"\n",
    "    Loads and preprocessed data for the MR dataset.\n",
    "    Returns input vectors, labels, vocabulary, and inverse vocabulary.\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    u_text, i_text, y_train, y_valid, u_len, i_len, u2_len, i2_len, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num \\\n",
    "        , reid_user_train, reid_item_train, reid_user_valid, reid_item_valid = \\\n",
    "        load_data_and_labels(train_data, valid_data, user_review, item_review, user_rid, item_rid, stopwords)\n",
    "    print \"load data done\"\n",
    "    u_text = pad_sentences(u_text, u_len, u2_len)\n",
    "    reid_user_train, reid_user_valid = pad_reviewid(reid_user_train, reid_user_valid, u_len, item_num + 1)\n",
    "\n",
    "    print \"pad user done\"\n",
    "    i_text = pad_sentences(i_text, i_len, i2_len)\n",
    "    reid_item_train, reid_item_valid = pad_reviewid(reid_item_train, reid_item_valid, i_len, user_num + 1)\n",
    "\n",
    "    print \"pad item done\"\n",
    "\n",
    "    user_voc = [xx for x in u_text.itervalues() for xx in x]\n",
    "    item_voc = [xx for x in i_text.itervalues() for xx in x]\n",
    "\n",
    "    vocabulary_user, vocabulary_inv_user, vocabulary_item, vocabulary_inv_item = build_vocab(user_voc, item_voc)\n",
    "    print len(vocabulary_user)\n",
    "    print len(vocabulary_item)\n",
    "    u_text, i_text = build_input_data(u_text, i_text, vocabulary_user, vocabulary_item)\n",
    "    y_train = np.array(y_train)\n",
    "    y_valid = np.array(y_valid)\n",
    "    uid_train = np.array(uid_train)\n",
    "    uid_valid = np.array(uid_valid)\n",
    "    iid_train = np.array(iid_train)\n",
    "    iid_valid = np.array(iid_valid)\n",
    "    reid_user_train = np.array(reid_user_train)\n",
    "    reid_user_valid = np.array(reid_user_valid)\n",
    "    reid_item_train = np.array(reid_item_train)\n",
    "    reid_item_valid = np.array(reid_item_valid)\n",
    "\n",
    "    return [u_text, i_text, y_train, y_valid, vocabulary_user, vocabulary_inv_user, vocabulary_item,\n",
    "            vocabulary_inv_item, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num, reid_user_train,\n",
    "            reid_item_train, reid_user_valid, reid_item_valid]\n",
    "\n",
    "\n",
    "def load_data_and_labels(train_data, valid_data, user_review, item_review, user_rid, item_rid, stopwords):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "\n",
    "\n",
    "    f_train = open(train_data, \"r\")\n",
    "    f1 = open(user_review)\n",
    "    f2 = open(item_review)\n",
    "    f3 = open(user_rid)\n",
    "    f4 = open(item_rid)\n",
    "\n",
    "    user_reviews = pickle.load(f1)\n",
    "    item_reviews = pickle.load(f2)\n",
    "    user_rids = pickle.load(f3)\n",
    "    item_rids = pickle.load(f4)\n",
    "\n",
    "    reid_user_train = []\n",
    "    reid_item_train = []\n",
    "    uid_train = []\n",
    "    iid_train = []\n",
    "    y_train = []\n",
    "    u_text = {}\n",
    "    u_rid = {}\n",
    "    i_text = {}\n",
    "    i_rid = {}\n",
    "    i = 0\n",
    "    for line in f_train:\n",
    "        i = i + 1\n",
    "        line = line.split(',')\n",
    "        uid_train.append(int(line[0]))\n",
    "        iid_train.append(int(line[1]))\n",
    "        if u_text.has_key(int(line[0])):\n",
    "            reid_user_train.append(u_rid[int(line[0])])\n",
    "        else:\n",
    "            u_text[int(line[0])] = []\n",
    "            for s in user_reviews[int(line[0])]:\n",
    "                s1 = clean_str(s)\n",
    "                s1 = s1.split(\" \")\n",
    "                u_text[int(line[0])].append(s1)\n",
    "            u_rid[int(line[0])] = []\n",
    "            for s in user_rids[int(line[0])]:\n",
    "                u_rid[int(line[0])].append(int(s))\n",
    "            reid_user_train.append(u_rid[int(line[0])])\n",
    "\n",
    "        if i_text.has_key(int(line[1])):\n",
    "            reid_item_train.append(i_rid[int(line[1])])  #####write here\n",
    "        else:\n",
    "            i_text[int(line[1])] = []\n",
    "            for s in item_reviews[int(line[1])]:\n",
    "                s1 = clean_str(s)\n",
    "                s1 = s1.split(\" \")\n",
    "\n",
    "                i_text[int(line[1])].append(s1)\n",
    "            i_rid[int(line[1])] = []\n",
    "            for s in item_rids[int(line[1])]:\n",
    "                i_rid[int(line[1])].append(int(s))\n",
    "            reid_item_train.append(i_rid[int(line[1])])\n",
    "        y_train.append(float(line[2]))\n",
    "    print \"valid\"\n",
    "    reid_user_valid = []\n",
    "    reid_item_valid = []\n",
    "\n",
    "    uid_valid = []\n",
    "    iid_valid = []\n",
    "    y_valid = []\n",
    "    f_valid = open(valid_data)\n",
    "    for line in f_valid:\n",
    "        line = line.split(',')\n",
    "        uid_valid.append(int(line[0]))\n",
    "        iid_valid.append(int(line[1]))\n",
    "        if u_text.has_key(int(line[0])):\n",
    "            reid_user_valid.append(u_rid[int(line[0])])\n",
    "        else:\n",
    "            u_text[int(line[0])] = [['<PAD/>']]\n",
    "            u_rid[int(line[0])] = [int(0)]\n",
    "            reid_user_valid.append(u_rid[int(line[0])])\n",
    "\n",
    "        if i_text.has_key(int(line[1])):\n",
    "            reid_item_valid.append(i_rid[int(line[1])])\n",
    "        else:\n",
    "            i_text[int(line[1])] = [['<PAD/>']]\n",
    "            i_rid[int(line[1])] = [int(0)]\n",
    "            reid_item_valid.append(i_rid[int(line[1])])\n",
    "\n",
    "        y_valid.append(float(line[2]))\n",
    "    print \"len\"\n",
    "\n",
    "\n",
    "    review_num_u = np.array([len(x) for x in u_text.itervalues()])\n",
    "    x = np.sort(review_num_u)\n",
    "    u_len = x[int(0.9 * len(review_num_u)) - 1]\n",
    "    review_len_u = np.array([len(j) for i in u_text.itervalues() for j in i])\n",
    "    x2 = np.sort(review_len_u)\n",
    "    u2_len = x2[int(0.9 * len(review_len_u)) - 1]\n",
    "\n",
    "    review_num_i = np.array([len(x) for x in i_text.itervalues()])\n",
    "    y = np.sort(review_num_i)\n",
    "    i_len = y[int(0.9 * len(review_num_i)) - 1]\n",
    "    review_len_i = np.array([len(j) for i in i_text.itervalues() for j in i])\n",
    "    y2 = np.sort(review_len_i)\n",
    "    i2_len = y2[int(0.9 * len(review_len_i)) - 1]\n",
    "    print \"u_len:\", u_len\n",
    "    print \"i_len:\", i_len\n",
    "    print \"u2_len:\", u2_len\n",
    "    print \"i2_len:\", i2_len\n",
    "    user_num = len(u_text)\n",
    "    item_num = len(i_text)\n",
    "    print \"user_num:\", user_num\n",
    "    print \"item_num:\", item_num\n",
    "    return [u_text, i_text, y_train, y_valid, u_len, i_len, u2_len, i2_len, uid_train,\n",
    "            iid_train, uid_valid, iid_valid, user_num,\n",
    "            item_num, reid_user_train, reid_item_train, reid_user_valid, reid_item_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"toy\"\n",
    "\n",
    "print directory\n",
    "\n",
    "# tf.flags.DEFINE_string(\"valid_data\", \"NARRE/data/\" + directory + \"/\" + directory + \"_valid.csv\", \" Data for validation\")\n",
    "# tf.flags.DEFINE_string(\"test_data\", \"NARRE/data/\" + directory + \"/\" + directory + \"_test.csv\", \"Data for testing\")\n",
    "# tf.flags.DEFINE_string(\"train_data\", \"NARRE/data/\" + directory + \"/\" + directory + \"_train.csv\", \"Data for training\")\n",
    "# tf.flags.DEFINE_string(\"user_review\", \"NARRE/data/\" + directory + \"/user_review\", \"User's reviews\")\n",
    "# tf.flags.DEFINE_string(\"item_review\", \"NARRE/data/\" + directory + \"/item_review\", \"Item's reviews\")\n",
    "# tf.flags.DEFINE_string(\"user_review_id\", \"NARRE/data/\" + directory + \"/user_rid\", \"user_review_id\")\n",
    "# tf.flags.DEFINE_string(\"item_review_id\", \"NARRE/data/\" + directory + \"/item_rid\", \"item_review_id\")\n",
    "# tf.flags.DEFINE_string(\"stopwords\", \"NARRE/data/stopwords\", \"stopwords\")\n",
    "\n",
    "train_data = \"./data/\" + directory + \"/\" + directory + \"_train.csv\"\n",
    "valid_data = \"./data/\" + directory + \"/\" + directory + \"_valid.csv\"\n",
    "test_data = \"./data/\" + directory + \"/\" + directory + \"_test.csv\"\n",
    "user_review = \"./data/\" + directory + \"/user_review\"\n",
    "item_review = \"./data/\" + directory + \"/item_review\"\n",
    "user_review_id = \"./data/\" + directory + \"/user_rid\"\n",
    "item_review_id = \"./data/\" + directory + \"/item_rid\"\n",
    "stopwords = None\n",
    "\n",
    "\n",
    "TPS_DIR = './data/' + directory\n",
    "\n",
    "u_text, i_text, y_train, y_valid, vocabulary_user, vocabulary_inv_user, vocabulary_item, \\\n",
    "vocabulary_inv_item, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num, reid_user_train, reid_item_train, reid_user_valid, reid_item_valid = \\\n",
    "    load_data(train_data, valid_data, user_review, item_review, user_review_id,\n",
    "              item_review_id, stopwords)\n",
    "\n",
    "np.random.seed(2017)\n",
    "\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "\n",
    "userid_train = uid_train[shuffle_indices]\n",
    "itemid_train = iid_train[shuffle_indices]\n",
    "y_train = y_train[shuffle_indices]\n",
    "reid_user_train = reid_user_train[shuffle_indices]\n",
    "reid_item_train = reid_item_train[shuffle_indices]\n",
    "\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_valid = y_valid[:, np.newaxis]\n",
    "\n",
    "userid_train = userid_train[:, np.newaxis]\n",
    "itemid_train = itemid_train[:, np.newaxis]\n",
    "userid_valid = uid_valid[:, np.newaxis]\n",
    "itemid_valid = iid_valid[:, np.newaxis]\n",
    "\n",
    "batches_train = list(\n",
    "    zip(userid_train, itemid_train, reid_user_train, reid_item_train, y_train))\n",
    "batches_test = list(zip(userid_valid, itemid_valid, reid_user_valid, reid_item_valid, y_valid))\n",
    "print 'write begin'\n",
    "output = open(os.path.join(TPS_DIR, directory + '.train'), 'wb')\n",
    "pickle.dump(batches_train, output)\n",
    "output.close()\n",
    "\n",
    "output = open(os.path.join(TPS_DIR, directory + '.test'), 'wb')\n",
    "pickle.dump(batches_test, output)\n",
    "output.close()\n",
    "\n",
    "para = {}\n",
    "para['user_num'] = user_num\n",
    "para['item_num'] = item_num\n",
    "para['review_num_u'] = u_text[0].shape[0]\n",
    "para['review_num_i'] = i_text[0].shape[0]\n",
    "para['review_len_u'] = u_text[1].shape[1]\n",
    "para['review_len_i'] = i_text[1].shape[1]\n",
    "para['user_vocab'] = vocabulary_user\n",
    "para['item_vocab'] = vocabulary_item\n",
    "para['train_length'] = len(y_train)\n",
    "para['test_length'] = len(y_valid)\n",
    "para['u_text'] = u_text\n",
    "para['i_text'] = i_text\n",
    "output = open(os.path.join(TPS_DIR, directory + '.para'), 'wb')\n",
    "# Pickle dictionary using protocol 0.\n",
    "pickle.dump(para, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NARRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:07:30.388253Z",
     "start_time": "2020-07-15T16:07:29.564504Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NARRE\n",
    "@author:\n",
    "Chong Chen (cstchenc@163.com)\n",
    "\n",
    "@ created:\n",
    "27/8/2017\n",
    "@references:\n",
    "Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma. 2018. Neural Attentional Rating Regression with Review-level Explanations. In WWW'18.\n",
    "'''\n",
    "class NARRE(object):\n",
    "    def __init__(\n",
    "            self, review_num_u, review_num_i, review_len_u, review_len_i, user_num, item_num, num_classes,\n",
    "            user_vocab_size, item_vocab_size, n_latent, embedding_id, attention_size,\n",
    "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        self.input_u = tf.placeholder(tf.int32, [None, review_num_u, review_len_u], name=\"input_u\")\n",
    "        self.input_i = tf.placeholder(tf.int32, [None, review_num_i, review_len_i], name=\"input_i\")\n",
    "        self.input_reuid = tf.placeholder(tf.int32, [None, review_num_u], name='input_reuid')\n",
    "        self.input_reiid = tf.placeholder(tf.int32, [None, review_num_i], name='input_reuid')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "        self.input_uid = tf.placeholder(tf.int32, [None, 1], name=\"input_uid\")\n",
    "        self.input_iid = tf.placeholder(tf.int32, [None, 1], name=\"input_iid\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.drop0 = tf.placeholder(tf.float32, name=\"dropout0\")\n",
    "        iidW = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidW\")\n",
    "        uidW = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidW\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"user_embedding\"):\n",
    "            self.W1 = tf.Variable(\n",
    "                tf.random_uniform([user_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W1\")\n",
    "            self.embedded_user = tf.nn.embedding_lookup(self.W1, self.input_u)\n",
    "            self.embedded_users = tf.expand_dims(self.embedded_user, -1)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"item_embedding\"):\n",
    "            self.W2 = tf.Variable(\n",
    "                tf.random_uniform([item_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W2\")\n",
    "            self.embedded_item = tf.nn.embedding_lookup(self.W2, self.input_i)\n",
    "            self.embedded_items = tf.expand_dims(self.embedded_item, -1)\n",
    "\n",
    "\n",
    "        pooled_outputs_u = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"user_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                self.embedded_users = tf.reshape(self.embedded_users, [-1, review_len_u, embedding_size, 1])\n",
    "\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_users,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, review_len_u - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_u.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "#         self.h_pool_u = tf.concat(3,pooled_outputs_u)\n",
    "        \n",
    "#         self.h_pool_flat_u = tf.reshape(self.h_pool_u, [-1, review_num_u, num_filters_total])\n",
    "\n",
    "        self.h_pool_u = pooled_outputs_u\n",
    "\n",
    "        self.h_pool_flat_u = tf.reshape(self.h_pool_u, [-1, review_num_u, num_filters_total])\n",
    "\n",
    "        pooled_outputs_i = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"item_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                self.embedded_items = tf.reshape(self.embedded_items, [-1, review_len_i, embedding_size, 1])\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_items,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, review_len_i - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_i.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        \n",
    "#         self.h_pool_i = tf.concat(3,pooled_outputs_i)\n",
    "#         self.h_pool_flat_i = tf.reshape(self.h_pool_i, [-1, review_num_i, num_filters_total])\n",
    "        self.h_pool_i = pooled_outputs_i\n",
    "        self.h_pool_flat_i = tf.reshape(self.h_pool_i, [-1, review_num_i, num_filters_total])\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop_u = tf.nn.dropout(self.h_pool_flat_u, 1.0)\n",
    "            self.h_drop_i = tf.nn.dropout(self.h_pool_flat_i, 1.0)\n",
    "        with tf.name_scope(\"attention\"):\n",
    "            Wau = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wau')\n",
    "            Wru = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wru')\n",
    "            Wpu = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpu')\n",
    "            bau = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bau\")\n",
    "            bbu = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbu\")\n",
    "            self.iid_a = tf.nn.relu(tf.nn.embedding_lookup(iidW, self.input_reuid))\n",
    "            \n",
    "            ##################\n",
    "#             print self.h_drop_u\n",
    "#             print Wau\n",
    "#             print self.iid_a\n",
    "#             print Wru\n",
    "#             print bau\n",
    "#             print Wpu\n",
    "#             print bbu\n",
    "            \n",
    "            \n",
    "            print self.h_drop_u.get_shape()\n",
    "            print Wau.get_shape()\n",
    "\n",
    "            sm111=tf.reshape(self.h_drop_u, shape=[-1, num_filters_total])\n",
    "            print num_filters_total\n",
    "            print tf.shape(sm111)\n",
    "            sm11=tf.matmul(sm111, Wau)\n",
    "            sm1=tf.reshape(sm11,shape=[-1,review_num_u,attention_size])\n",
    "            sm2=tf.reshape(tf.matmul(tf.reshape(self.iid_a,shape=[-1,embedding_id]),Wru),shape=[-1,review_num_u,attention_size])\n",
    "            sm3=tf.nn.relu(sm1+sm2+bau)\n",
    "            print tf.shape(sm3)\n",
    "            print attention_size\n",
    "            self.u_j=tf.reshape(tf.matmul(tf.reshape(sm3,shape=[-1,attention_size]),Wpu),shape=[-1,review_num_u,1])+bbu\n",
    "            \n",
    "            ##################\n",
    "            \n",
    "#             self.u_j = tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "#                 tf.einsum('ajk,kl->ajl', self.h_drop_u, Wau) + tf.einsum('ajk,kl->ajl', self.iid_a, Wru) + bau),\n",
    "#                                              Wpu)+bbu  # None*u_len*1\n",
    "\n",
    "            self.u_a = tf.nn.softmax(self.u_j,1)  # none*u_len*1\n",
    "\n",
    "            print self.u_a\n",
    "\n",
    "            Wai = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wai')\n",
    "            Wri = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wri')\n",
    "            Wpi = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpi')\n",
    "            bai = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bai\")\n",
    "            bbi = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbi\")\n",
    "            self.uid_a = tf.nn.relu(tf.nn.embedding_lookup(uidW, self.input_reiid))\n",
    "#             self.i_j =tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "#                 tf.einsum('ajk,kl->ajl', self.h_drop_i, Wai) + tf.einsum('ajk,kl->ajl', self.uid_a, Wri) + bai),\n",
    "#                                              Wpi)+bbi\n",
    "            \n",
    "            #####################\n",
    "        \n",
    "            sm111 = tf.reshape(self.h_drop_i, shape=[-1, num_filters_total])\n",
    "            sm11 = tf.matmul(sm111, Wai)\n",
    "            sm1 = tf.reshape(sm11, shape=[-1, review_num_i, attention_size])\n",
    "            sm2 = tf.reshape(tf.matmul(tf.reshape(self.uid_a, shape=[-1, embedding_id]), Wri), shape=[-1, review_num_i,\n",
    "                                                                                                      attention_size])\n",
    "            sm3 = tf.nn.relu(sm1+sm2+bai)\n",
    "            self.i_j = tf.reshape(tf.matmul(tf.reshape(sm3, shape=[-1, attention_size]), Wpi), shape=[-1, review_num_i, 1]) + bbi\n",
    "            \n",
    "            #####################\n",
    "            \n",
    "\n",
    "            self.i_a = tf.nn.softmax(self.i_j,1)  # none*len*1\n",
    "\n",
    "            l2_loss += tf.nn.l2_loss(Wau)\n",
    "            l2_loss += tf.nn.l2_loss(Wru)\n",
    "            l2_loss += tf.nn.l2_loss(Wri)\n",
    "            l2_loss += tf.nn.l2_loss(Wai)\n",
    "\n",
    "        with tf.name_scope(\"add_reviews\"):\n",
    "            self.u_feas = tf.reduce_sum(tf.multiply(self.u_a, self.h_drop_u), 1)\n",
    "            self.u_feas = tf.nn.dropout(self.u_feas, self.dropout_keep_prob)\n",
    "            self.i_feas = tf.reduce_sum(tf.multiply(self.i_a, self.h_drop_i), 1)\n",
    "            self.i_feas = tf.nn.dropout(self.i_feas, self.dropout_keep_prob)\n",
    "        with tf.name_scope(\"get_fea\"):\n",
    "\n",
    "            iidmf = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidmf\")\n",
    "            uidmf = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidmf\")\n",
    "\n",
    "            self.uid = tf.nn.embedding_lookup(uidmf,self.input_uid)\n",
    "            self.iid = tf.nn.embedding_lookup(iidmf,self.input_iid)\n",
    "            self.uid = tf.reshape(self.uid,[-1,embedding_id])\n",
    "            self.iid = tf.reshape(self.iid,[-1,embedding_id])\n",
    "            Wu = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wu')\n",
    "            bu = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bu\")\n",
    "            self.u_feas = tf.matmul(self.u_feas, Wu)+self.uid + bu\n",
    "\n",
    "            Wi = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wi')\n",
    "            bi = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bi\")\n",
    "            self.i_feas = tf.matmul(self.i_feas, Wi) +self.iid+ bi\n",
    "\n",
    "       \n",
    "\n",
    "        with tf.name_scope('ncf'):\n",
    "\n",
    "            self.FM = tf.multiply(self.u_feas, self.i_feas)\n",
    "            self.FM = tf.nn.relu(self.FM)\n",
    "\n",
    "            self.FM=tf.nn.dropout(self.FM,self.dropout_keep_prob)\n",
    "\n",
    "            Wmul=tf.Variable(\n",
    "                tf.random_uniform([n_latent, 1], -0.1, 0.1), name='wmul')\n",
    "\n",
    "            self.mul=tf.matmul(self.FM,Wmul)\n",
    "            self.score=tf.reduce_sum(self.mul,1,keep_dims=True)\n",
    "\n",
    "            self.uidW2 = tf.Variable(tf.constant(0.1, shape=[user_num + 2]), name=\"uidW2\")\n",
    "            self.iidW2 = tf.Variable(tf.constant(0.1, shape=[item_num + 2]), name=\"iidW2\")\n",
    "            self.u_bias = tf.gather(self.uidW2, self.input_uid)\n",
    "            self.i_bias = tf.gather(self.iidW2, self.input_iid)\n",
    "            self.Feature_bias = self.u_bias + self.i_bias\n",
    "\n",
    "            self.bised = tf.Variable(tf.constant(0.1), name='bias')\n",
    "\n",
    "            self.predictions = self.score + self.Feature_bias + self.bised\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.l2_loss(tf.subtract(self.predictions, self.input_y))\n",
    "\n",
    "            self.loss = losses + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.mae = tf.reduce_mean(tf.abs(tf.subtract(self.predictions, self.input_y)))\n",
    "            self.accuracy =tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN NARRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:07:31.315844Z",
     "start_time": "2020-07-15T16:07:30.456923Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NARRE\n",
    "@author:\n",
    "Chong Chen (cstchenc@163.com)\n",
    "\n",
    "@ created:\n",
    "27/8/2017\n",
    "@references:\n",
    "Chong Chen, Min Zhang, Yiqun Liu, and Shaoping Ma. 2018. Neural Attentional Rating Regression with Review-level Explanations. In WWW'18.\n",
    "'''\n",
    "def fc_layer(x, num_units, use_relu=True):\n",
    "    \"\"\"\n",
    "    Create a fully-connected layer\n",
    "    :param x: input from previous layer\n",
    "    :param num_units: number of hidden units in the fully-connected layer\n",
    "    :param name: layer name\n",
    "    :param use_relu: boolean to add ReLU non-linearity (or not)\n",
    "    :return: The output array\n",
    "    \"\"\"\n",
    "    in_dim = x.shape\n",
    "    print in_dim\n",
    "#     print x\n",
    "    W = tf.Variable(tf.random_uniform([in_dim[1].value, num_units], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.random_uniform([num_units,], -0.1, 0.1))\n",
    "    \n",
    "    \n",
    "    layer = tf.matmul(x, W)\n",
    "    layer += b\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    return layer\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "class NARRE_dnn(object):\n",
    "    def __init__(\n",
    "            self, review_num_u, review_num_i, review_len_u, review_len_i, user_num, item_num, num_classes,\n",
    "            user_vocab_size, item_vocab_size, n_latent, embedding_id, attention_size,\n",
    "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0,\n",
    "            dnn_layers_size=64, n_dnn_layers=3 ):\n",
    "        self.input_u = tf.placeholder(tf.int32, [None, review_num_u, review_len_u], name=\"input_u\")\n",
    "        self.input_i = tf.placeholder(tf.int32, [None, review_num_i, review_len_i], name=\"input_i\")\n",
    "        self.input_reuid = tf.placeholder(tf.int32, [None, review_num_u], name='input_reuid')\n",
    "        self.input_reiid = tf.placeholder(tf.int32, [None, review_num_i], name='input_reuid')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "        self.input_uid = tf.placeholder(tf.int32, [None, 1], name=\"input_uid\")\n",
    "        self.input_iid = tf.placeholder(tf.int32, [None, 1], name=\"input_iid\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.drop0 = tf.placeholder(tf.float32, name=\"dropout0\")\n",
    "        iidW = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidW\")\n",
    "        uidW = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidW\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"user_embedding\"):\n",
    "            self.W1 = tf.Variable(\n",
    "                tf.random_uniform([user_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W1\")\n",
    "            self.embedded_user = tf.nn.embedding_lookup(self.W1, self.input_u)\n",
    "            self.embedded_users = tf.expand_dims(self.embedded_user, -1)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"item_embedding\"):\n",
    "            self.W2 = tf.Variable(\n",
    "                tf.random_uniform([item_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W2\")\n",
    "            self.embedded_item = tf.nn.embedding_lookup(self.W2, self.input_i)\n",
    "            self.embedded_items = tf.expand_dims(self.embedded_item, -1)\n",
    "\n",
    "\n",
    "        pooled_outputs_u = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"user_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                self.embedded_users = tf.reshape(self.embedded_users, [-1, review_len_u, embedding_size, 1])\n",
    "\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_users,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, review_len_u - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_u.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool_u = tf.concat(pooled_outputs_u, 3)\n",
    "        \n",
    "        self.h_pool_flat_u = tf.reshape(self.h_pool_u, [-1, review_num_u, num_filters_total])\n",
    "\n",
    "#         self.h_pool_u = pooled_outputs_u\n",
    "\n",
    "#         self.h_pool_flat_u = tf.reshape(self.h_pool_u, [-1, review_num_u, num_filters_total])\n",
    "\n",
    "        pooled_outputs_i = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"item_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                self.embedded_items = tf.reshape(self.embedded_items, [-1, review_len_i, embedding_size, 1])\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_items,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, review_len_i - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_i.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        \n",
    "        self.h_pool_i = tf.concat(pooled_outputs_i,3)\n",
    "        self.h_pool_flat_i = tf.reshape(self.h_pool_i, [-1, review_num_i, num_filters_total])\n",
    "#         self.h_pool_i = pooled_outputs_i\n",
    "#         self.h_pool_flat_i = tf.reshape(self.h_pool_i, [-1, review_num_i, num_filters_total])\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop_u = tf.nn.dropout(self.h_pool_flat_u, 1.0)\n",
    "            self.h_drop_i = tf.nn.dropout(self.h_pool_flat_i, 1.0)\n",
    "        with tf.name_scope(\"attention\"):\n",
    "            Wau = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wau')\n",
    "            Wru = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wru')\n",
    "            Wpu = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpu')\n",
    "            bau = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bau\")\n",
    "            bbu = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbu\")\n",
    "            self.iid_a = tf.nn.relu(tf.nn.embedding_lookup(iidW, self.input_reuid))\n",
    "            \n",
    "            ##################\n",
    "#             print self.h_drop_u\n",
    "#             print Wau\n",
    "#             print self.iid_a\n",
    "#             print Wru\n",
    "#             print bau\n",
    "#             print Wpu\n",
    "#             print bbu\n",
    "            \n",
    "            \n",
    "            print self.h_drop_u.get_shape()\n",
    "            print Wau.get_shape()\n",
    "\n",
    "            sm111=tf.reshape(self.h_drop_u, shape=[-1, num_filters_total])\n",
    "            print num_filters_total\n",
    "            print tf.shape(sm111)\n",
    "            sm11=tf.matmul(sm111, Wau)\n",
    "            sm1=tf.reshape(sm11,shape=[-1,review_num_u,attention_size])\n",
    "            sm2=tf.reshape(tf.matmul(tf.reshape(self.iid_a,shape=[-1,embedding_id]),Wru),shape=[-1,review_num_u,attention_size])\n",
    "            sm3=tf.nn.relu(sm1+sm2+bau)\n",
    "            print tf.shape(sm3)\n",
    "            print attention_size\n",
    "            self.u_j=tf.reshape(tf.matmul(tf.reshape(sm3,shape=[-1,attention_size]),Wpu),shape=[-1,review_num_u,1])+bbu\n",
    "            \n",
    "            ##################\n",
    "            \n",
    "#             self.u_j = tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "#                 tf.einsum('ajk,kl->ajl', self.h_drop_u, Wau) + tf.einsum('ajk,kl->ajl', self.iid_a, Wru) + bau),\n",
    "#                                              Wpu)+bbu  # None*u_len*1\n",
    "\n",
    "            self.u_a = tf.nn.softmax(self.u_j,1)  # none*u_len*1\n",
    "\n",
    "            print self.u_a\n",
    "\n",
    "            Wai = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wai')\n",
    "            Wri = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wri')\n",
    "            Wpi = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpi')\n",
    "            bai = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bai\")\n",
    "            bbi = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbi\")\n",
    "            self.uid_a = tf.nn.relu(tf.nn.embedding_lookup(uidW, self.input_reiid))\n",
    "#             self.i_j =tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "#                 tf.einsum('ajk,kl->ajl', self.h_drop_i, Wai) + tf.einsum('ajk,kl->ajl', self.uid_a, Wri) + bai),\n",
    "#                                              Wpi)+bbi\n",
    "            \n",
    "            #####################\n",
    "        \n",
    "            sm111 = tf.reshape(self.h_drop_i, shape=[-1, num_filters_total])\n",
    "            sm11 = tf.matmul(sm111, Wai)\n",
    "            sm1 = tf.reshape(sm11, shape=[-1, review_num_i, attention_size])\n",
    "            sm2 = tf.reshape(tf.matmul(tf.reshape(self.uid_a, shape=[-1, embedding_id]), Wri), shape=[-1, review_num_i,\n",
    "                                                                                                      attention_size])\n",
    "            sm3 = tf.nn.relu(sm1+sm2+bai)\n",
    "            self.i_j = tf.reshape(tf.matmul(tf.reshape(sm3, shape=[-1, attention_size]), Wpi), shape=[-1, review_num_i, 1]) + bbi\n",
    "            \n",
    "            #####################\n",
    "            \n",
    "\n",
    "            self.i_a = tf.nn.softmax(self.i_j,1)  # none*len*1\n",
    "\n",
    "            l2_loss += tf.nn.l2_loss(Wau)\n",
    "            l2_loss += tf.nn.l2_loss(Wru)\n",
    "            l2_loss += tf.nn.l2_loss(Wri)\n",
    "            l2_loss += tf.nn.l2_loss(Wai)\n",
    "\n",
    "        with tf.name_scope(\"add_reviews\"):\n",
    "            self.u_feas = tf.reduce_sum(tf.multiply(self.u_a, self.h_drop_u), 1)\n",
    "            self.u_feas = tf.nn.dropout(self.u_feas, self.dropout_keep_prob)\n",
    "            self.i_feas = tf.reduce_sum(tf.multiply(self.i_a, self.h_drop_i), 1)\n",
    "            self.i_feas = tf.nn.dropout(self.i_feas, self.dropout_keep_prob)\n",
    "        with tf.name_scope(\"get_fea\"):\n",
    "\n",
    "            iidmf = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidmf\")\n",
    "            uidmf = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidmf\")\n",
    "\n",
    "            self.uid = tf.nn.embedding_lookup(uidmf,self.input_uid)\n",
    "            self.iid = tf.nn.embedding_lookup(iidmf,self.input_iid)\n",
    "            self.uid = tf.reshape(self.uid,[-1,embedding_id])\n",
    "            self.iid = tf.reshape(self.iid,[-1,embedding_id])\n",
    "            Wu = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wu')\n",
    "            bu = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bu\")\n",
    "            self.u_feas = tf.matmul(self.u_feas, Wu)+self.uid + bu\n",
    "\n",
    "            Wi = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wi')\n",
    "            bi = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bi\")\n",
    "            self.i_feas = tf.matmul(self.i_feas, Wi) +self.iid+ bi\n",
    "\n",
    "       \n",
    "\n",
    "        with tf.name_scope('ncf'):\n",
    "\n",
    "            self.FM = tf.multiply(self.u_feas, self.i_feas)\n",
    "            self.FM = tf.nn.relu(self.FM)\n",
    "\n",
    "            self.FM=tf.nn.dropout(self.FM,self.dropout_keep_prob)\n",
    "\n",
    "#             Wmul=tf.Variable(\n",
    "#                 tf.random_uniform([n_latent, 1], -0.1, 0.1), name='wmul')\n",
    "\n",
    "#             self.mul=tf.matmul(self.FM,Wmul)\n",
    "#             self.score=tf.reduce_sum(self.mul,1,keep_dims=True)\n",
    "            \n",
    "            print self.FM.shape\n",
    "            \n",
    "            ############## DNNN ##################\n",
    "            if not isinstance(dnn_layers_size, list):\n",
    "                dnn_layers_size = [dnn_layers_size for i in range(n_dnn_layers)]\n",
    "            \n",
    "            \n",
    "            self.x_dnn = fc_layer(self.FM, dnn_layers_size[0], use_relu=True)\n",
    "            self.x_dnn = tf.nn.dropout(self.x_dnn, self.dropout_keep_prob)\n",
    "            for layer in dnn_layers_size[1:]:\n",
    "                self.x_dnn = fc_layer(self.x_dnn, layer, use_relu=True)\n",
    "                # Apply dropout\n",
    "#                 self.x_dnn = tf.nn.dropout(self.x_dnn, self.dropout_keep_prob)\n",
    "            self.score = fc_layer(self.x_dnn, 1, use_relu=False)\n",
    "            ############## DNNN ##################\n",
    "\n",
    "            self.uidW2 = tf.Variable(tf.constant(0.1, shape=[user_num + 2]), name=\"uidW2\")\n",
    "            self.iidW2 = tf.Variable(tf.constant(0.1, shape=[item_num + 2]), name=\"iidW2\")\n",
    "            self.u_bias = tf.gather(self.uidW2, self.input_uid)\n",
    "            self.i_bias = tf.gather(self.iidW2, self.input_iid)\n",
    "            self.Feature_bias = self.u_bias + self.i_bias\n",
    "\n",
    "            self.bised = tf.Variable(tf.constant(0.1), name='bias')\n",
    "\n",
    "#             self.predictions = self.score + self.Feature_bias + self.bised\n",
    "            self.predictions = self.x_dnn + self.Feature_bias + self.bised\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.l2_loss(tf.subtract(self.predictions, self.input_y))\n",
    "\n",
    "            self.loss = losses + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.mae = tf.reduce_mean(tf.abs(tf.subtract(self.predictions, self.input_y)))\n",
    "            self.accuracy =tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention NARRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:07:32.625348Z",
     "start_time": "2020-07-15T16:07:31.599718Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Multi_NARRE\n",
    "Improved NARRE with fully connected layers in the prediction stage and\n",
    "attention mechinisem over the feature level.\n",
    "'''\n",
    "class Multi_NARRE(object):\n",
    "    def __init__(\n",
    "            self, review_num_u, review_num_i, review_len_u, review_len_i, user_num, item_num, num_classes,\n",
    "            user_vocab_size, item_vocab_size, n_latent, embedding_id, attention_size,\n",
    "            embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        self.input_u = tf.placeholder(tf.int32, [None, review_num_u, review_len_u], name=\"input_u\")\n",
    "        self.input_i = tf.placeholder(tf.int32, [None, review_num_i, review_len_i], name=\"input_i\")\n",
    "        self.input_reuid = tf.placeholder(tf.int32, [None, review_num_u], name='input_reuid')\n",
    "        self.input_reiid = tf.placeholder(tf.int32, [None, review_num_i], name='input_reuid')\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, 1], name=\"input_y\")\n",
    "        self.input_uid = tf.placeholder(tf.int32, [None, 1], name=\"input_uid\")\n",
    "        self.input_iid = tf.placeholder(tf.int32, [None, 1], name=\"input_iid\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.drop0 = tf.placeholder(tf.float32, name=\"dropout0\")\n",
    "        iidW = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidW\")\n",
    "        uidW = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidW\")\n",
    "\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.name_scope(\"user_embedding\"):\n",
    "            self.W1 = tf.Variable(\n",
    "                tf.random_uniform([user_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W1\")\n",
    "            self.embedded_user = tf.nn.embedding_lookup(self.W1, self.input_u)\n",
    "            self.embedded_users = tf.expand_dims(self.embedded_user, -1)\n",
    "\n",
    "\n",
    "        with tf.name_scope(\"item_embedding\"):\n",
    "            self.W2 = tf.Variable(\n",
    "                tf.random_uniform([item_vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W2\")\n",
    "            self.embedded_item = tf.nn.embedding_lookup(self.W2, self.input_i)\n",
    "            self.embedded_items = tf.expand_dims(self.embedded_item, -1)\n",
    "\n",
    "\n",
    "        pooled_outputs_u = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"user_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                self.embedded_users = tf.reshape(self.embedded_users, [-1, review_len_u, embedding_size, 1])\n",
    "\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_users,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, review_len_u - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_u.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool_u = tf.concat(pooled_outputs_u, 3)\n",
    "        \n",
    "        self.h_pool_flat_u = tf.reshape(self.h_pool_u, [-1, review_num_u, num_filters_total])\n",
    "\n",
    "        pooled_outputs_i = []\n",
    "\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"item_conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                self.embedded_items = tf.reshape(self.embedded_items, [-1, review_len_i, embedding_size, 1])\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_items,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, review_len_i - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs_i.append(pooled)\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool_i = tf.concat(pooled_outputs_i, 3)\n",
    "        self.h_pool_flat_i = tf.reshape(self.h_pool_i, [-1, review_num_i, num_filters_total])\n",
    "        \n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop_u = tf.nn.dropout(self.h_pool_flat_u, 1.0)\n",
    "            self.h_drop_i = tf.nn.dropout(self.h_pool_flat_i, 1.0)\n",
    "        with tf.name_scope(\"attention\"):\n",
    "            Wau = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wau')\n",
    "            Wru = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wru')\n",
    "            Wpu = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpu')\n",
    "            bau = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bau\")\n",
    "            bbu = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbu\")\n",
    "            self.iid_a = tf.nn.relu(tf.nn.embedding_lookup(iidW, self.input_reuid))\n",
    "            \n",
    "            ##################\n",
    "#             print self.h_drop_u\n",
    "#             print Wau\n",
    "#             print self.iid_a\n",
    "#             print Wru\n",
    "#             print bau\n",
    "#             print Wpu\n",
    "#             print bbu\n",
    "            \n",
    "            \n",
    "            print self.h_drop_u.get_shape()\n",
    "            print Wau.get_shape()\n",
    "\n",
    "            sm111=tf.reshape(self.h_drop_u, shape=[-1, num_filters_total])\n",
    "            print num_filters_total\n",
    "            print tf.shape(sm111)\n",
    "            sm11=tf.matmul(sm111, Wau)\n",
    "            sm1=tf.reshape(sm11,shape=[-1,review_num_u,attention_size])\n",
    "            sm2=tf.reshape(tf.matmul(tf.reshape(self.iid_a,shape=[-1,embedding_id]),Wru),shape=[-1,review_num_u,attention_size])\n",
    "            sm3=tf.nn.relu(sm1+sm2+bau)\n",
    "            print tf.shape(sm3)\n",
    "            print attention_size\n",
    "            self.u_j=tf.reshape(tf.matmul(tf.reshape(sm3,shape=[-1,attention_size]),Wpu),shape=[-1,review_num_u,1])+bbu\n",
    "            \n",
    "            ##################\n",
    "            \n",
    "#             self.u_j = tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "#                 tf.einsum('ajk,kl->ajl', self.h_drop_u, Wau) + tf.einsum('ajk,kl->ajl', self.iid_a, Wru) + bau),\n",
    "#                                              Wpu)+bbu  # None*u_len*1\n",
    "\n",
    "            self.u_a = tf.nn.softmax(self.u_j,1)  # none*u_len*1\n",
    "\n",
    "            print self.u_a\n",
    "\n",
    "            Wai = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, attention_size], -0.1, 0.1), name='Wai')\n",
    "            Wri = tf.Variable(\n",
    "                tf.random_uniform([embedding_id, attention_size], -0.1, 0.1), name='Wri')\n",
    "            Wpi = tf.Variable(\n",
    "                tf.random_uniform([attention_size, 1], -0.1, 0.1), name='Wpi')\n",
    "            bai = tf.Variable(tf.constant(0.1, shape=[attention_size]), name=\"bai\")\n",
    "            bbi = tf.Variable(tf.constant(0.1, shape=[1]), name=\"bbi\")\n",
    "            self.uid_a = tf.nn.relu(tf.nn.embedding_lookup(uidW, self.input_reiid))\n",
    "#             self.i_j =tf.einsum('ajk,kl->ajl', tf.nn.relu(\n",
    "#                 tf.einsum('ajk,kl->ajl', self.h_drop_i, Wai) + tf.einsum('ajk,kl->ajl', self.uid_a, Wri) + bai),\n",
    "#                                              Wpi)+bbi\n",
    "            \n",
    "            #####################\n",
    "        \n",
    "            sm111 = tf.reshape(self.h_drop_i, shape=[-1, num_filters_total])\n",
    "            sm11 = tf.matmul(sm111, Wai)\n",
    "            sm1 = tf.reshape(sm11, shape=[-1, review_num_i, attention_size])\n",
    "            sm2 = tf.reshape(tf.matmul(tf.reshape(self.uid_a, shape=[-1, embedding_id]), Wri), shape=[-1, review_num_i,\n",
    "                                                                                                      attention_size])\n",
    "            sm3 = tf.nn.relu(sm1+sm2+bai)\n",
    "            self.i_j = tf.reshape(tf.matmul(tf.reshape(sm3, shape=[-1, attention_size]), Wpi), shape=[-1, review_num_i, 1]) + bbi\n",
    "            \n",
    "            #####################\n",
    "            \n",
    "\n",
    "            self.i_a = tf.nn.softmax(self.i_j,1)  # none*len*1\n",
    "\n",
    "            l2_loss += tf.nn.l2_loss(Wau)\n",
    "            l2_loss += tf.nn.l2_loss(Wru)\n",
    "            l2_loss += tf.nn.l2_loss(Wri)\n",
    "            l2_loss += tf.nn.l2_loss(Wai)\n",
    "\n",
    "        with tf.name_scope(\"add_reviews\"):\n",
    "            self.u_feas = tf.reduce_sum(tf.multiply(self.u_a, self.h_drop_u), 1)\n",
    "            self.u_feas = tf.nn.dropout(self.u_feas, self.dropout_keep_prob)\n",
    "            self.i_feas = tf.reduce_sum(tf.multiply(self.i_a, self.h_drop_i), 1)\n",
    "            self.i_feas = tf.nn.dropout(self.i_feas, self.dropout_keep_prob)\n",
    "        with tf.name_scope(\"get_fea\"):\n",
    "\n",
    "            iidmf = tf.Variable(tf.random_uniform([item_num + 2, embedding_id], -0.1, 0.1), name=\"iidmf\")\n",
    "            uidmf = tf.Variable(tf.random_uniform([user_num + 2, embedding_id], -0.1, 0.1), name=\"uidmf\")\n",
    "\n",
    "            self.uid = tf.nn.embedding_lookup(uidmf,self.input_uid)\n",
    "            self.iid = tf.nn.embedding_lookup(iidmf,self.input_iid)\n",
    "            self.uid = tf.reshape(self.uid,[-1,embedding_id])\n",
    "            self.iid = tf.reshape(self.iid,[-1,embedding_id])\n",
    "            Wu = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wu')\n",
    "            bu = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bu\")\n",
    "            self.u_feas = tf.matmul(self.u_feas, Wu)+self.uid + bu\n",
    "\n",
    "            Wi = tf.Variable(\n",
    "                tf.random_uniform([num_filters_total, n_latent], -0.1, 0.1), name='Wi')\n",
    "            bi = tf.Variable(tf.constant(0.1, shape=[n_latent]), name=\"bi\")\n",
    "            self.i_feas = tf.matmul(self.i_feas, Wi) +self.iid+ bi\n",
    "            \n",
    "            print self.i_feas.shape\n",
    "            print self.u_feas.shape\n",
    "       \n",
    "\n",
    "        with tf.name_scope('ncf'):\n",
    "\n",
    "            self.FM = tf.multiply(self.u_feas, self.i_feas)\n",
    "            self.FM = tf.nn.relu(self.FM)\n",
    "\n",
    "            self.FM=tf.nn.dropout(self.FM,self.dropout_keep_prob)\n",
    "\n",
    "            print self.FM.shape \n",
    "            ##########################\n",
    "            \n",
    "            # Trainable parameters\n",
    "            FM_hidden_size = self.FM.shape[1]\n",
    "            w_omega = tf.get_variable(name=\"w_omega\", shape=[FM_hidden_size, attention_size], initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "            b_omega = tf.get_variable(name=\"b_omega\", shape=[attention_size], initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "            u_omega = tf.get_variable(name=\"u_omega\", shape=[attention_size, attention_size], initializer=tf.random_uniform_initializer(-0.1,0.1))\n",
    "\n",
    "            with tf.name_scope('v'):\n",
    "                # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
    "                #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
    "                v = tf.tanh(tf.tensordot(self.FM, w_omega, axes=1) + b_omega)\n",
    "\n",
    "#             # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "            vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "            alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "            print alphas.shape\n",
    "#             print vu.shape\n",
    "#             print alphas.shape\n",
    "\n",
    "            # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "            self.FM = tf.math.multiply(self.FM, alphas)\n",
    "            \n",
    "            print self.FM.shape\n",
    "            \n",
    "            \n",
    "            ##########################\n",
    "            # First fully connected net\n",
    "#             with tf.variable_scope('hidden1'):\n",
    "#                 weights = tf.get_variable(\"W\", [32, 16],\n",
    "#                       initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "#                 biases = tf.get_variable(\"b\", [16], initializer=tf.constant_initializer(0.1))\n",
    "#                 hidden1 = tf.nn.relu(tf.matmul(self.FM, weights) + biases)\n",
    "\n",
    "#             # Second fully connected net\n",
    "#             with tf.variable_scope('hidden2'):\n",
    "#                 weights = tf.get_variable(\"W\", [16, 16],\n",
    "#                       initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "#                 biases = tf.get_variable(\"b\", [16], initializer=tf.constant_initializer(0.1))\n",
    "#                 hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "#             # Linear\n",
    "#             with tf.variable_scope('softmax_linear'):\n",
    "#                 weights = tf.get_variable(\"W\", [16, 1],\n",
    "#                       initializer=tf.random_uniform_initializer(-0.1, 0.1))\n",
    "#                 biases = tf.get_variable(\"b\", [1], initializer=tf.constant_initializer(0.1))\n",
    "#                 self.score = tf.nn.relu(tf.matmul(hidden2, weights) + biases)\n",
    "\n",
    "        \n",
    "        \n",
    "        ###############################\n",
    "            Wmul=tf.Variable(\n",
    "                tf.random_uniform([n_latent, 1], -0.1, 0.1), name='wmul')\n",
    "\n",
    "            self.mul=tf.matmul(self.FM,Wmul)\n",
    "            self.score=tf.reduce_sum(self.mul,1,keep_dims=True)\n",
    "        ###############################\n",
    "            self.uidW2 = tf.Variable(tf.constant(0.1, shape=[user_num + 2]), name=\"uidW2\")\n",
    "            self.iidW2 = tf.Variable(tf.constant(0.1, shape=[item_num + 2]), name=\"iidW2\")\n",
    "            self.u_bias = tf.gather(self.uidW2, self.input_uid)\n",
    "            self.i_bias = tf.gather(self.iidW2, self.input_iid)\n",
    "            self.Feature_bias = self.u_bias + self.i_bias\n",
    "\n",
    "            self.bised = tf.Variable(tf.constant(0.1), name='bias')\n",
    "\n",
    "            self.predictions = self.score + self.Feature_bias + self.bised\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.l2_loss(tf.subtract(self.predictions, self.input_y))\n",
    "\n",
    "            self.loss = losses + l2_reg_lambda * l2_loss\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.mae = tf.reduce_mean(tf.abs(tf.subtract(self.predictions, self.input_y)))\n",
    "            self.accuracy =tf.sqrt(tf.reduce_mean(tf.square(tf.subtract(self.predictions, self.input_y))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:07:33.320590Z",
     "start_time": "2020-07-15T16:07:33.276149Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NARRE\n",
    "@author:\n",
    "Chong Chen (cstchenc@163.com)\n",
    "\n",
    "@ created:\n",
    "27/8/2017\n",
    "@references:\n",
    "\n",
    "'''\n",
    "\n",
    "def train_step(u_batch, i_batch, uid, iid, reuid, reiid, y_batch,batch_num):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_reuid: reuid,\n",
    "        deep.input_reiid: reiid,\n",
    "        deep.drop0: 0.8,\n",
    "\n",
    "        deep.dropout_keep_prob: dropout_keep_prob\n",
    "    }\n",
    "    _, step, loss, accuracy, mae, u_a, i_a = sess.run(\n",
    "        [train_op, global_step, deep.loss, deep.accuracy, deep.mae, deep.u_a, deep.i_a],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    #print(\"{}: step {}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, batch_num, loss, accuracy, mae))\n",
    "    return loss, accuracy, mae, u_a, i_a\n",
    "\n",
    "\n",
    "def dev_step(u_batch, i_batch, uid, iid, reuid, reiid, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        deep.input_u: u_batch,\n",
    "        deep.input_i: i_batch,\n",
    "        deep.input_y: y_batch,\n",
    "        deep.input_uid: uid,\n",
    "        deep.input_iid: iid,\n",
    "        deep.input_reuid: reuid,\n",
    "        deep.input_reiid: reiid,\n",
    "        deep.drop0: 1.0,\n",
    "        deep.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, loss, accuracy, mae, preds = sess.run(\n",
    "        [global_step, deep.loss, deep.accuracy, deep.mae, deep.score],\n",
    "        feed_dict)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    # print(\"{}: step{}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, step, loss, accuracy, mae))\n",
    "\n",
    "    return [loss, accuracy, mae, preds]\n",
    "\n",
    "\n",
    "def write_summary(value, tag, summary_writer, global_step):\n",
    "    \"\"\"Write a single summary value to tensorboard\"\"\"\n",
    "    summary = tf.Summary()\n",
    "    summary.value.add(tag=tag, simple_value=value)\n",
    "    summary_writer.add_summary(summary, global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:28:26.009281Z",
     "start_time": "2020-07-15T16:28:10.336695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "19412\n",
      "11924\n",
      "11\n",
      "224\n",
      "23\n",
      "224\n"
     ]
    }
   ],
   "source": [
    "directory = 'toys'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model Hyperparameters\n",
    "word2vec = \"./data/google.bin\"\n",
    "train_data = \"./data/\" + directory + \"/\" + directory + \".train\"\n",
    "valid_data = \"./data/\" + directory + \"/\" + directory + \".test\"\n",
    "para_data = \"./data/\" + directory + \"/\" + directory + \".para\"\n",
    "save_path = \"./improvment/\" + directory + \"/best_validation_rmse\"\n",
    "\n",
    "# word2vec = \"data/google.bin\" # \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "# train_data = \"data/\" + directory + \"/\" + directory + \".train\" # \" Data for validation\")\n",
    "# valid_data = \"data/\" + directory + \"/\" + directory + \".test\" # \"Data parameters\")\n",
    "# para_data = \"data/\" + directory + \"/\" + directory + \".para\" # \"Data for training\")\n",
    "\n",
    "logs_path = \"./logs/\" +  datetime.datetime.now().strftime('run-%Y-%m-%d-%H-%M')\n",
    "os.makedirs(logs_path)\n",
    "embedding_dim = 300\n",
    "filter_sizes = \"3\"\n",
    "num_filters = 100\n",
    "dropout_keep_prob = 0.3\n",
    "l2_reg_lambda = 0.001\n",
    "embedding_id = 32\n",
    "latent_size = 32\n",
    "attention_size = 32\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "num_epochs = 40\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False\n",
    "allow_growth = True\n",
    "\n",
    "\n",
    "# Loading Data\n",
    "print(\"Loading data...\")\n",
    "pkl_file = open(para_data, 'rb')\n",
    "para = pickle.load(pkl_file)\n",
    "user_num = para['user_num']\n",
    "item_num = para['item_num']\n",
    "review_num_u = para['review_num_u']\n",
    "review_num_i = para['review_num_i']\n",
    "review_len_u = para['review_len_u']\n",
    "review_len_i = para['review_len_i']\n",
    "vocabulary_user = para['user_vocab']\n",
    "vocabulary_item = para['item_vocab']\n",
    "train_length = para['train_length']\n",
    "test_length = para['test_length']\n",
    "u_text = para['u_text']\n",
    "i_text = para['i_text']\n",
    "# pkl_file.close()\n",
    "np.random.seed(2017)\n",
    "random_seed = 2017\n",
    "print user_num\n",
    "print item_num\n",
    "print review_num_u\n",
    "print review_len_u\n",
    "print review_num_i\n",
    "print review_len_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:30:10.880309Z",
     "start_time": "2020-07-15T16:28:49.755317Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('toys', 'attention')\n",
      "(?, 11, 100)\n",
      "(100, 32)\n",
      "100\n",
      "Tensor(\"attention/Shape:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"attention/Shape_1:0\", shape=(3,), dtype=int32)\n",
      "32\n",
      "Tensor(\"attention/transpose_1:0\", shape=(?, 11, 1), dtype=float32)\n",
      "(?, 32)\n",
      "(?, 32)\n",
      "(?, 32)\n",
      "(?, 32)\n",
      "(?, 32)\n",
      "19412\n",
      "11924\n",
      "Load word2vec u file data/google.bin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/ipykernel_launcher.py:108: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec i file data/google.bin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/ipykernel_launcher.py:134: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39049\n",
      "Start training\n",
      "Finished first batch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-c7b0b9d64544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m                     \u001b[0mi_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                     \u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_rmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_mae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreiid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                     \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                     \u001b[0msum_tloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-31139a85e63f>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(u_batch, i_batch, uid, iid, reuid, reiid, y_batch, batch_num)\u001b[0m\n\u001b[1;32m     28\u001b[0m     _, step, loss, accuracy, mae, u_a, i_a = sess.run(\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi_a\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         feed_dict)\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mtime_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misoformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#print(\"{}: step {}, loss {:g}, rmse {:g},mae {:g}\".format(time_str, batch_num, loss, accuracy, mae))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/rdo/.conda/envs/rs_project/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# tf.flags.DEFINE_string(\"word2vec\", \"../data/google.bin\", \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "# tf.flags.DEFINE_string(\"valid_data\", \"../data/\" + directory + \"/\" + directory + \".test\", \" Data for validation\")\n",
    "# tf.flags.DEFINE_string(\"para_data\", \"../data/\" + directory + \"/\" + directory + \".para\", \"Data parameters\")\n",
    "# tf.flags.DEFINE_string(\"train_data\", \"../data/\" + directory + \"/\" + directory + \".train\", \"Data for training\")\n",
    "# # ==================================================\n",
    "\n",
    "# # Model Hyperparameters\n",
    "# # tf.flags.DEFINE_string(\"word2vec\", \"./data/rt-polaritydata/google.bin\", \"Word2vec file with pre-trained embeddings (default: None)\")\n",
    "# tf.flags.DEFINE_integer(\"embedding_dim\", 300, \"Dimensionality of character embedding \")\n",
    "# tf.flags.DEFINE_string(\"filter_sizes\", \"3\", \"Comma-separated filter sizes \")\n",
    "# tf.flags.DEFINE_integer(\"num_filters\", 100, \"Number of filters per filter size\")\n",
    "# tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability \")\n",
    "# tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.001, \"L2 regularizaion lambda\")\n",
    "# # Training parameters\n",
    "# tf.flags.DEFINE_integer(\"batch_size\", 100, \"Batch Size \")\n",
    "# tf.flags.DEFINE_integer(\"num_epochs\", 40, \"Number of training epochs \")\n",
    "# # Misc Parameters\n",
    "# tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "# tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "# narre_version = 'original'\n",
    "# narre_version = 'dnn'\n",
    "narre_version = 'attention'\n",
    "\n",
    "# save_path = \"../models/model_\" + directory  + \"_\" + narre_version\n",
    "\n",
    "\n",
    "print (directory, narre_version)\n",
    "\n",
    "model_name = 'original_paper'\n",
    "narre_params = {}\n",
    "\n",
    "if narre_version == 'original':\n",
    "    narre = NARRE\n",
    "if narre_version == 'dnn':\n",
    "    narre = NARRE_dnn\n",
    "if narre_version == 'attention':\n",
    "    narre = Multi_NARRE\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=allow_soft_placement,\n",
    "        log_device_placement=log_device_placement)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        \n",
    "        deep = narre(\n",
    "            review_num_u=review_num_u,\n",
    "            review_num_i=review_num_i,\n",
    "            review_len_u=review_len_u,\n",
    "            review_len_i=review_len_i,\n",
    "            user_num=user_num,\n",
    "            item_num=item_num,\n",
    "            num_classes=1,\n",
    "            user_vocab_size=len(vocabulary_user),\n",
    "            item_vocab_size=len(vocabulary_item),\n",
    "            embedding_size=embedding_dim,\n",
    "            embedding_id=32,\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda,\n",
    "            attention_size=32,\n",
    "            n_latent=32,\n",
    "            **narre_params)\n",
    "        \n",
    "        writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "        tf.set_random_seed(random_seed)\n",
    "        print user_num\n",
    "        print item_num\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "        # optimizer = tf.train.AdagradOptimizer(learning_rate=0.01, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "        optimizer = tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss, global_step=global_step)\n",
    "\n",
    "        train_op = optimizer  # .apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        if word2vec:\n",
    "            # initial matrix with random uniform\n",
    "            u = 0\n",
    "            initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_user), embedding_dim))\n",
    "            # load any vectors from the word2vec\n",
    "            print(\"Load word2vec u file {}\\n\".format(word2vec))\n",
    "            with open(word2vec, \"rb\") as f:\n",
    "                header = f.readline()\n",
    "                vocab_size, layer1_size = map(int, header.split())\n",
    "                binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "                for line in xrange(vocab_size):\n",
    "                    word = []\n",
    "                    while True:\n",
    "                        ch = f.read(1)\n",
    "                        if ch == ' ':\n",
    "                            word = ''.join(word)\n",
    "                            break\n",
    "                        if ch != '\\n':\n",
    "                            word.append(ch)\n",
    "                    idx = 0\n",
    "\n",
    "                    if word in vocabulary_user:\n",
    "                        u = u + 1\n",
    "                        idx = vocabulary_user[word]\n",
    "                        initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                    else:\n",
    "                        f.read(binary_len)\n",
    "            sess.run(deep.W1.assign(initW))\n",
    "            initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_item), embedding_dim))\n",
    "            # load any vectors from the word2vec\n",
    "            print(\"Load word2vec i file {}\\n\".format(word2vec))\n",
    "\n",
    "            item = 0\n",
    "            with open(word2vec, \"rb\") as f:\n",
    "                header = f.readline()\n",
    "                vocab_size, layer1_size = map(int, header.split())\n",
    "                binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "                for line in xrange(vocab_size):\n",
    "                    word = []\n",
    "                    while True:\n",
    "                        ch = f.read(1)\n",
    "                        if ch == ' ':\n",
    "                            word = ''.join(word)\n",
    "                            break\n",
    "                        if ch != '\\n':\n",
    "                            word.append(ch)\n",
    "                    idx = 0\n",
    "                    if word in vocabulary_item:\n",
    "                        item = item + 1\n",
    "                        idx = vocabulary_item[word]\n",
    "                        initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                    else:\n",
    "                        f.read(binary_len)\n",
    "\n",
    "            sess.run(deep.W2.assign(initW))\n",
    "            print item\n",
    "\n",
    "        best_mae = 5\n",
    "        best_rmse = 5\n",
    "        train_mae = 0\n",
    "        train_rmse = 0\n",
    "        sum_tloss = 0\n",
    "        early_stop = 3\n",
    "        stopping_step = 0\n",
    "        should_stop = False\n",
    "\n",
    "        pkl_file = open(train_data, 'rb')\n",
    "\n",
    "        train_data = pickle.load(pkl_file)\n",
    "\n",
    "        train_data = np.array(train_data)\n",
    "        pkl_file.close()\n",
    "\n",
    "        pkl_file = open(valid_data, 'rb')\n",
    "\n",
    "        test_data = pickle.load(pkl_file)\n",
    "        test_data = np.array(test_data)\n",
    "        pkl_file.close()\n",
    "\n",
    "        data_size_train = len(train_data)\n",
    "        data_size_test = len(test_data)\n",
    "        batch_size = batch_size\n",
    "        ll = int(len(train_data) / batch_size)\n",
    "        print 'Start training'\n",
    "        start = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            if not should_stop:\n",
    "                # Shuffle the data at each epoch\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size_train))\n",
    "                shuffled_data = train_data[shuffle_indices]\n",
    "                for batch_num in range(ll):\n",
    "\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_train)\n",
    "                    data_train = shuffled_data[start_index:end_index]\n",
    "\n",
    "                    uid, iid, reuid, reiid, y_batch = zip(*data_train)\n",
    "                    u_batch = []\n",
    "                    i_batch = []\n",
    "                    for i in range(len(uid)):\n",
    "                        u_batch.append(u_text[uid[i][0]])\n",
    "                        i_batch.append(i_text[iid[i][0]])\n",
    "                    u_batch = np.array(u_batch)\n",
    "                    i_batch = np.array(i_batch)\n",
    "\n",
    "                    t_loss, t_rmse, t_mae, u_a, i_a = train_step(u_batch, i_batch, uid, iid, reuid, reiid, y_batch,batch_num)\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    sum_tloss += t_loss\n",
    "                    train_rmse += t_rmse\n",
    "                    train_mae += t_mae\n",
    "                    if epoch == 0 and batch_num == 0:\n",
    "                        print ('Finished first batch')\n",
    "                    if batch_num % 500 == 0 and batch_num > 1:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        print batch_num\n",
    "\n",
    "                        loss_s = 0\n",
    "                        accuracy_s = 0\n",
    "                        mae_s = 0\n",
    "\n",
    "                        ll_test = int(len(test_data) / batch_size) + 1\n",
    "                        for batch_num in range(ll_test):\n",
    "                            start_index = batch_num * batch_size\n",
    "                            end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                            data_test = test_data[start_index:end_index]\n",
    "\n",
    "                            userid_valid, itemid_valid, reuid, reiid, y_valid = zip(*data_test)\n",
    "                            u_valid = []\n",
    "                            i_valid = []\n",
    "                            for i in range(len(userid_valid)):\n",
    "                                u_valid.append(u_text[userid_valid[i][0]])\n",
    "                                i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                            u_valid = np.array(u_valid)\n",
    "                            i_valid = np.array(i_valid)\n",
    "\n",
    "                            loss, accuracy, mae, _ = dev_step(u_valid, i_valid, userid_valid, itemid_valid, reuid, reiid,\n",
    "                                                           y_valid)\n",
    "                            loss_s = loss_s + len(u_valid) * loss\n",
    "                            accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                            mae_s = mae_s + len(u_valid) * mae\n",
    "\n",
    "                        print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                         np.sqrt(\n",
    "                                                                                             accuracy_s / test_length),\n",
    "                                                                                         mae_s / test_length))\n",
    "\n",
    "\n",
    "                        rmse = np.sqrt(accuracy_s / test_length)\n",
    "                        mae = mae_s / test_length\n",
    "\n",
    "                        print(\"\")\n",
    "                        write_summary(loss_s / test_length, \"validation/loss\", writer, current_step)\n",
    "                        write_summary(rmse, \"validation/rmse\", writer, current_step)\n",
    "                        write_summary(mae, \"validation/mae\", writer, current_step)\n",
    "\n",
    "                print str(epoch) + ':\\n'\n",
    "                print(\"\\nEvaluation:\")\n",
    "                print \"train:rmse,mae:\", train_rmse / ll, train_mae / ll\n",
    "                u_a = np.reshape(u_a[0], (1, -1))\n",
    "                i_a = np.reshape(i_a[0], (1, -1))\n",
    "\n",
    "\n",
    "                write_summary(sum_tloss / ll, \"train/loss\", writer, current_step)\n",
    "                write_summary(train_rmse / ll, \"train/rmse\", writer, current_step)\n",
    "                write_summary(train_mae / ll, \"train/mae\", writer, current_step)\n",
    "\n",
    "                print u_a\n",
    "                print i_a\n",
    "                train_rmse = 0\n",
    "                train_mae = 0\n",
    "                sum_tloss = 0\n",
    "\n",
    "\n",
    "        \n",
    "            loss_s = 0\n",
    "            accuracy_s = 0\n",
    "            mae_s = 0  \n",
    "\n",
    "            ll_test = int(len(test_data) / batch_size) + 1\n",
    "            for batch_num in range(ll_test):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                data_test = test_data[start_index:end_index]\n",
    "\n",
    "                userid_valid, itemid_valid, reuid, reiid, y_valid = zip(*data_test)\n",
    "                u_valid = []\n",
    "                i_valid = []\n",
    "                for i in range(len(userid_valid)):\n",
    "                    u_valid.append(u_text[userid_valid[i][0]])\n",
    "                    i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                u_valid = np.array(u_valid)\n",
    "                i_valid = np.array(i_valid)\n",
    "\n",
    "                loss, accuracy, mae, preds = dev_step(u_valid, i_valid, userid_valid, itemid_valid, reuid, reiid, y_valid)\n",
    "                loss_s = loss_s + len(u_valid) * loss\n",
    "                accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                mae_s = mae_s + len(u_valid) * mae\n",
    "            print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                             np.sqrt(accuracy_s / test_length),\n",
    "                                                                             mae_s / test_length))\n",
    "            rmse = np.sqrt(accuracy_s / test_length)\n",
    "            mae = mae_s / test_length\n",
    "\n",
    "            if best_rmse > rmse:\n",
    "                stopping_step = 0\n",
    "                best_rmse = rmse\n",
    "                best_preds = preds\n",
    "                # Save all variables of the TensorFlow graph to file.\n",
    "                # saver.save(sess=sess, save_path=save_path, global_step=global_step)\n",
    "            else:\n",
    "                stopping_step += 1\n",
    "                if stopping_step >= early_stop:\n",
    "                    should_stop = True\n",
    "                    print (\"Early stopping is trigger at epoch: {} loss:{}\".format(epoch,loss_s / test_length))\n",
    "\n",
    "            if best_mae > mae:\n",
    "                best_mae = mae\n",
    "        print(\"\")\n",
    "        end = time.time()\n",
    "        training_time = end-start\n",
    "        filename = narre_version + '_' + directory + '_' + datetime.datetime.now().strftime('run-%Y-%m-%d-%H-%M') + '.csv'\n",
    "        results = {'model':narre_version, 'data':directory, 'training time':training_time, 'rmse':best_rmse, 'mae':best_mae, 'preds':best_preds,\n",
    "                  'embedding dim': embedding_dim, 'filter sizes': filter_sizes, 'num of filters': num_filters,\n",
    "                   'dropout prob': dropout_keep_prob,'l2_reg': l2_reg_lambda, 'embedding_id': embedding_id,\n",
    "                   'latent size': latent_size, 'attention_size': attention_size, 'batch size': batch_size, 'epochs': num_epochs}\n",
    "        \n",
    "\n",
    "        with open('./output/' + directory + '/'+ filename, 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for key, value in results.items():\n",
    "                writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "\n",
    "\n",
    "\n",
    "# initial matrix with random uniform\n",
    "u = 0\n",
    "initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_user), embedding_dim))\n",
    "# load any vectors from the word2vec\n",
    "print(\"Load word2vec u file {}\\n\".format(word2vec))\n",
    "with open(word2vec, \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in xrange(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1)\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        idx = 0\n",
    "\n",
    "        if word in vocabulary_user:\n",
    "            u = u + 1\n",
    "            idx = vocabulary_user[word]\n",
    "            initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "        else:\n",
    "            f.read(binary_len)\n",
    "sess.run(deep.W1.assign(initW))\n",
    "initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_item), embedding_dim))\n",
    "# load any vectors from the word2vec\n",
    "print(\"Load word2vec i file {}\\n\".format(word2vec))\n",
    "\n",
    "item = 0\n",
    "with open(word2vec, \"rb\") as f:\n",
    "    header = f.readline()\n",
    "    vocab_size, layer1_size = map(int, header.split())\n",
    "    binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "    for line in xrange(vocab_size):\n",
    "        word = []\n",
    "        while True:\n",
    "            ch = f.read(1)\n",
    "            if ch == ' ':\n",
    "                word = ''.join(word)\n",
    "                break\n",
    "            if ch != '\\n':\n",
    "                word.append(ch)\n",
    "        idx = 0\n",
    "        if word in vocabulary_item:\n",
    "            item = item + 1\n",
    "            idx = vocabulary_item[word]\n",
    "            initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "        else:\n",
    "            f.read(binary_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0710 11:35:59.856458 140261217179392 plugin_event_accumulator.py:294] Found more than one graph event per run, or there was a metagraph containing a graph_def, as well as one or more graph events.  Overwriting the graph with the newest event.\n",
      "TensorBoard 1.14.0 at http://132.72.44.124:6006/ (Press CTRL+C to quit)\n",
      "I0710 11:36:01.658716 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:01] \"GET / HTTP/1.1\" 200 -\n",
      "I0710 11:36:02.636636 140261301073664 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:02] \"GET /font-roboto/oMMgfZMQthOryQo9n22dcuvvDin1pK8aKteLpeZ5c0A.woff2 HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.029244 140261301073664 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/environment HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.041244 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugins_listing HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.046973 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/experiments HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.062640 140260026590976 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.114626 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /font-roboto/RxZJdnzeo3R5zSexge8UUZBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.124862 140261301073664 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/environment HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.142883 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugins_listing HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.144084 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/experiments HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.157700 140260026590976 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.536982 140260026590976 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/tags HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.545950 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /font-roboto/d-6IYplOFocCacKzxwXSOJBw1xU1rKptJj_0jans920.woff2 HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.640129 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/tags HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.860531 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-17-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.889960 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=.&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.907664 140261301073664 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.909321 140260026590976 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-15-25&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.910166 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=.&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.910695 140259916502784 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=run-2020-07-09-17-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:04.967315 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.053981 140261301073664 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=run-2020-07-09-17-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.054712 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=.&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.056624 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.057300 140260026590976 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=.&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.057910 140259916502784 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=.&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.066730 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=run-2020-07-09-17-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.111016 140261301073664 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-17-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.111845 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.118743 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-15-25&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.148596 140259916502784 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=run-2020-07-09-17-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.149010 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=.&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.149573 140260026590976 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:05.194540 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:05] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.164688 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/environment HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.165517 140259916502784 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/experiments HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.166471 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.183475 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/plugins_listing HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.297379 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/plugin/scalars/tags HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.399122 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.413516 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:36:34.414299 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:36:34] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.201406 140260034983680 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/plugins_listing HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.204631 140259916502784 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/runs HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.219403 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/environment HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.220927 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/experiments HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.374538 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/plugin/scalars/tags HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.529201 140261376575232 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Floss&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.530045 140261259126528 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Fmae&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "I0710 11:37:04.530849 140259916502784 _internal.py:113] 132.72.201.8 - - [10/Jul/2020 11:37:04] \"GET /data/plugin/scalars/scalars?tag=validation%2Frmse&run=run-2020-07-09-22-20&experiment= HTTP/1.1\" 200 -\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python -m tensorboard.main --logdir=./logs/ --host=132.72.44.124"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_project",
   "language": "python",
   "name": "rs_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "707px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
