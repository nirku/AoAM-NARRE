{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:07:25.682174Z",
     "start_time": "2020-07-15T16:07:20.842702Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import csv\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# our modules\n",
    "sys.path.insert(0, './models')\n",
    "from loaddata import loader\n",
    "from preprocess_utils import process\n",
    "from NARRE import NARRE\n",
    "from NARRE_DNN import NARRE_dnn\n",
    "from NARRE_Attention import Multi_NARRE\n",
    "\n",
    "from train import train_step, dev_step, write_summary\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'kindle'\n",
    "data_path = './data'\n",
    "TPS_DIR = os.path.join(data_path, dataset)\n",
    "TP_file = 'Kindle_Store_5.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Started.\n",
      "Data Loader Finished...\n",
      "Files Saved at ./data/kindle\n"
     ]
    }
   ],
   "source": [
    "data_loader = loader(TPS_DIR, TP_file)\n",
    "data_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set paths to dataset \n",
    "train_data = os.path.join(TPS_DIR, 'train.csv')\n",
    "valid_data = os.path.join(TPS_DIR, 'valid.csv')\n",
    "test_data = os.path.join(TPS_DIR, 'test.csv')\n",
    "user_review = os.path.join(TPS_DIR, 'user_review')\n",
    "item_review = os.path.join(TPS_DIR, 'item_review')\n",
    "user_review_id = os.path.join(TPS_DIR, 'user_rid')\n",
    "item_review_id = os.path.join(TPS_DIR, 'item_rid')\n",
    "stopwords = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid\n",
      "len\n",
      "u_len: 24\n",
      "i_len: 39\n",
      "u2_len: 234\n",
      "i2_len: 234\n",
      "user_num: 139816\n",
      "item_num: 98823\n",
      "load data done\n",
      "pad user done\n",
      "pad item done\n",
      "235921\n",
      "259763\n"
     ]
    }
   ],
   "source": [
    "# load all elements from data\n",
    "u_text, i_text, y_train, y_valid, vocabulary_user, vocabulary_inv_user, vocabulary_item, \\\n",
    "vocabulary_inv_item, uid_train, iid_train, uid_valid, iid_valid, user_num, item_num, reid_user_train, reid_item_train, reid_user_valid, reid_item_valid = \\\n",
    "    process(train_data, valid_data, user_review, item_review, user_review_id,\n",
    "              item_review_id, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train/validation and create batches\n",
    "np.random.seed(2020)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y_train)))\n",
    "\n",
    "userid_train = uid_train[shuffle_indices]\n",
    "itemid_train = iid_train[shuffle_indices]\n",
    "y_train = y_train[shuffle_indices]\n",
    "reid_user_train = reid_user_train[shuffle_indices]\n",
    "reid_item_train = reid_item_train[shuffle_indices]\n",
    "\n",
    "y_train = y_train[:, np.newaxis]\n",
    "y_valid = y_valid[:, np.newaxis]\n",
    "\n",
    "userid_train = userid_train[:, np.newaxis]\n",
    "itemid_train = itemid_train[:, np.newaxis]\n",
    "userid_valid = uid_valid[:, np.newaxis]\n",
    "itemid_valid = iid_valid[:, np.newaxis]\n",
    "\n",
    "batches_train = list(\n",
    "    zip(userid_train, itemid_train, reid_user_train, reid_item_train, y_train))\n",
    "batches_test = list(zip(userid_valid, itemid_valid, reid_user_valid, reid_item_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write begin\n"
     ]
    }
   ],
   "source": [
    "# save files for train / test\n",
    "print 'write begin'\n",
    "output = open(os.path.join(TPS_DIR, dataset + '.train'), 'wb')\n",
    "pickle.dump(batches_train, output)\n",
    "output.close()\n",
    "\n",
    "output = open(os.path.join(TPS_DIR, dataset + '.test'), 'wb')\n",
    "pickle.dump(batches_test, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dictionary of all parameters\n",
    "para = {}\n",
    "para['user_num'] = user_num\n",
    "para['item_num'] = item_num\n",
    "para['review_num_u'] = u_text[0].shape[0]\n",
    "para['review_num_i'] = i_text[0].shape[0]\n",
    "para['review_len_u'] = u_text[1].shape[1]\n",
    "para['review_len_i'] = i_text[1].shape[1]\n",
    "para['user_vocab'] = vocabulary_user\n",
    "para['item_vocab'] = vocabulary_item\n",
    "para['train_length'] = len(y_train)\n",
    "para['test_length'] = len(y_valid)\n",
    "para['u_text'] = u_text\n",
    "para['i_text'] = i_text\n",
    "output = open(os.path.join(TPS_DIR, dataset + '.para'), 'wb')\n",
    "pickle.dump(para, output)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:28:26.009281Z",
     "start_time": "2020-07-15T16:28:10.336695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Num. of Users:  139816\n",
      "Num. of Items:  98823\n",
      "Num. of User Reviews:  24\n",
      "Length of User Reviews:  234\n",
      "Num. of Item Reviews:  39\n",
      "Length of Item Reviews:  234\n"
     ]
    }
   ],
   "source": [
    "# define paths\n",
    "word2vec = os.path.join(data_path, 'google.bin')\n",
    "train_data = os.path.join(TPS_DIR, dataset + \".train\")\n",
    "valid_data = os.path.join(TPS_DIR, dataset + \".test\")\n",
    "para_data = os.path.join(TPS_DIR, dataset + \".para\")\n",
    "\n",
    "# create log file\n",
    "logs_path = \"./logs/\" +  datetime.datetime.now().strftime('run-%Y-%m-%d-%H-%M')\n",
    "os.makedirs(logs_path)\n",
    "\n",
    "# Model Hyperparameters\n",
    "embedding_dim = 300\n",
    "filter_sizes = \"3\"\n",
    "num_filters = 100\n",
    "dropout_keep_prob = 0.3\n",
    "l2_reg_lambda = 0.001\n",
    "embedding_id = 32\n",
    "latent_size = 32\n",
    "attention_size = 32\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 100\n",
    "num_epochs = 1\n",
    "\n",
    "# Misc Parameters\n",
    "allow_soft_placement = True\n",
    "log_device_placement = False\n",
    "allow_growth = True\n",
    "\n",
    "\n",
    "# Loading Data\n",
    "print(\"Loading data...\")\n",
    "pkl_file = open(para_data, 'rb')\n",
    "para = pickle.load(pkl_file)\n",
    "user_num = para['user_num']\n",
    "item_num = para['item_num']\n",
    "review_num_u = para['review_num_u']\n",
    "review_num_i = para['review_num_i']\n",
    "review_len_u = para['review_len_u']\n",
    "review_len_i = para['review_len_i']\n",
    "vocabulary_user = para['user_vocab']\n",
    "vocabulary_item = para['item_vocab']\n",
    "train_length = para['train_length']\n",
    "test_length = para['test_length']\n",
    "u_text = para['u_text']\n",
    "i_text = para['i_text']\n",
    "pkl_file.close()\n",
    "np.random.seed(2017)\n",
    "random_seed = 2017\n",
    "\n",
    "print 'Num. of Users: ', user_num\n",
    "print 'Num. of Items: ', item_num\n",
    "print 'Num. of User Reviews: ', review_num_u\n",
    "print 'Length of User Reviews: ', review_len_u\n",
    "print 'Num. of Item Reviews: ', review_num_i\n",
    "print 'Length of Item Reviews: ', review_len_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "narre_version = 'attention'\n",
    "narre_params = {}\n",
    "\n",
    "\n",
    "if narre_version == 'original':\n",
    "    narre = NARRE\n",
    "if narre_version == 'dnn':\n",
    "    narre = NARRE_dnn\n",
    "if narre_version == 'attention':\n",
    "    narre = Multi_NARRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-15T16:30:10.880309Z",
     "start_time": "2020-07-15T16:28:49.755317Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./models/NARRE_Attention.py:12: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ./models/NARRE_Attention.py:21: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "w1:  (235921, 300)\n",
      "embedded_user:  Tensor(\"user_embedding/embedding_lookup/Identity:0\", shape=(?, 24, 234, 300), dtype=float32)\n",
      "embedded_users:  Tensor(\"user_embedding/ExpandDims:0\", shape=(?, 24, 234, 300, 1), dtype=float32)\n",
      "w2:  (259763, 300)\n",
      "embedded_item:  Tensor(\"item_embedding/embedding_lookup/Identity:0\", shape=(?, 39, 234, 300), dtype=float32)\n",
      "embedded_items:  Tensor(\"item_embedding/ExpandDims:0\", shape=(?, 39, 234, 300, 1), dtype=float32)\n",
      "WARNING:tensorflow:From ./models/NARRE_Attention.py:56: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "WARNING:tensorflow:From ./models/NARRE_Attention.py:69: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From ./models/NARRE_Attention.py:111: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(?, 24, 100)\n",
      "(100, 32)\n",
      "100\n",
      "Tensor(\"attention/Shape:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"attention/Shape_1:0\", shape=(3,), dtype=int32)\n",
      "32\n",
      "Tensor(\"attention/transpose_1:0\", shape=(?, 24, 1), dtype=float32)\n",
      "(?, 32)\n",
      "(?, 32)\n",
      "(?, 32)\n",
      "WARNING:tensorflow:From ./models/NARRE_Attention.py:231: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "(?, 32)\n",
      "(?, 32)\n",
      "WARNING:tensorflow:From ./models/NARRE_Attention.py:282: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "139816\n",
      "98823\n",
      "WARNING:tensorflow:From /home/nirku/.conda/envs/rs_project/lib/python2.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Load word2vec u file ./data/google.bin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirku/.conda/envs/rs_project/lib/python2.7/site-packages/ipykernel_launcher.py:69: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load word2vec i file ./data/google.bin\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirku/.conda/envs/rs_project/lib/python2.7/site-packages/ipykernel_launcher.py:95: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82259\n",
      "Start training\n",
      "Finished first batch\n",
      "\n",
      "Evaluation:\n",
      "500\n",
      "loss_valid 289.664, rmse_valid 2.40697, mae_valid 2.28501\n",
      "\n",
      "WARNING:tensorflow:From train.py:55: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "1000\n",
      "loss_valid 242.177, rmse_valid 2.20078, mae_valid 2.08434\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "1500\n",
      "loss_valid 178.284, rmse_valid 1.88822, mae_valid 1.77359\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2000\n",
      "loss_valid 115.998, rmse_valid 1.52295, mae_valid 1.40684\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "2500\n",
      "loss_valid 91.2951, rmse_valid 1.35101, mae_valid 1.24215\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "3000\n",
      "loss_valid 69.4568, rmse_valid 1.17831, mae_valid 1.07114\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "3500\n",
      "loss_valid 55.7925, rmse_valid 1.05598, mae_valid 0.936568\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "4000\n",
      "loss_valid 47.7686, rmse_valid 0.977045, mae_valid 0.842042\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "4500\n",
      "loss_valid 44.8663, rmse_valid 0.94691, mae_valid 0.804681\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "5000\n",
      "loss_valid 44.2165, rmse_valid 0.940043, mae_valid 0.796324\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "5500\n",
      "loss_valid 41.7497, rmse_valid 0.913462, mae_valid 0.763047\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "6000\n",
      "loss_valid 40.5223, rmse_valid 0.899942, mae_valid 0.749331\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "6500\n",
      "loss_valid 39.6688, rmse_valid 0.890418, mae_valid 0.738338\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "7000\n",
      "loss_valid 38.7552, rmse_valid 0.880115, mae_valid 0.724498\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "7500\n",
      "loss_valid 38.6897, rmse_valid 0.879372, mae_valid 0.723886\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "8000\n",
      "loss_valid 39.5078, rmse_valid 0.888643, mae_valid 0.734469\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "8500\n",
      "loss_valid 37.9139, rmse_valid 0.870532, mae_valid 0.711242\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "9000\n",
      "loss_valid 38.0633, rmse_valid 0.872259, mae_valid 0.714331\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "9500\n",
      "loss_valid 36.4074, rmse_valid 0.853074, mae_valid 0.689045\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "10000\n",
      "loss_valid 36.9702, rmse_valid 0.859661, mae_valid 0.695352\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "10500\n",
      "loss_valid 32.4132, rmse_valid 0.804902, mae_valid 0.605001\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "11000\n",
      "loss_valid 35.4504, rmse_valid 0.8418, mae_valid 0.671458\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "11500\n",
      "loss_valid 34.1831, rmse_valid 0.826593, mae_valid 0.644558\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "12000\n",
      "loss_valid 33.8881, rmse_valid 0.823016, mae_valid 0.64185\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "12500\n",
      "loss_valid 33.8255, rmse_valid 0.822208, mae_valid 0.64077\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "13000\n",
      "loss_valid 32.7313, rmse_valid 0.808807, mae_valid 0.618211\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "13500\n",
      "loss_valid 33.4488, rmse_valid 0.817643, mae_valid 0.634905\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "14000\n",
      "loss_valid 34.1455, rmse_valid 0.826133, mae_valid 0.648971\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "14500\n",
      "loss_valid 32.8568, rmse_valid 0.810325, mae_valid 0.6139\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "15000\n",
      "loss_valid 32.7886, rmse_valid 0.809472, mae_valid 0.61866\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "15500\n",
      "loss_valid 32.3348, rmse_valid 0.803858, mae_valid 0.608714\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "16000\n",
      "loss_valid 33.8237, rmse_valid 0.822144, mae_valid 0.640149\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "16500\n",
      "loss_valid 32.6864, rmse_valid 0.80816, mae_valid 0.617058\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "17000\n",
      "loss_valid 32.1382, rmse_valid 0.801348, mae_valid 0.599954\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "17500\n",
      "loss_valid 32.4404, rmse_valid 0.805085, mae_valid 0.606864\n",
      "\n",
      "0:\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "train:rmse,mae: 0.8634822522728999 0.6507546699875117\n",
      "[[1.4461539e-20 5.0388020e-27 1.3850219e-12 7.1027079e-34 7.7454992e-26\n",
      "  9.9426997e-01 4.6985674e-06 9.4037234e-05 2.3698695e-27 2.6773496e-15\n",
      "  1.5042799e-07 8.1026548e-23 7.1160800e-26 2.0051814e-22 5.1559880e-21\n",
      "  1.6773631e-20 3.9074111e-23 4.8880286e-14 5.6312066e-03 1.0901401e-22\n",
      "  1.0901401e-22 1.0901401e-22 1.0901401e-22 1.0901401e-22]]\n",
      "[[4.3974761e-02 4.6130758e-02 4.9740415e-02 5.6623470e-02 3.9764438e-02\n",
      "  4.5793161e-02 4.1234862e-02 4.7694847e-02 3.9284986e-02 4.9060877e-02\n",
      "  4.7572196e-02 4.8147228e-02 5.5860575e-02 6.1295900e-02 5.0091911e-02\n",
      "  5.4549795e-02 4.4049241e-02 4.9066741e-02 4.1552752e-02 4.0487342e-02\n",
      "  4.8014343e-02 5.2295752e-07 5.2295752e-07 5.2295752e-07 5.2295752e-07\n",
      "  5.2295752e-07 5.2295752e-07 5.2295752e-07 5.2295752e-07 5.2295752e-07\n",
      "  5.2295752e-07 5.2295752e-07 5.2295752e-07 5.2295752e-07 5.2295752e-07\n",
      "  5.2295752e-07 5.2295752e-07 5.2295752e-07 5.2295752e-07]]\n",
      "loss_valid 31.8922, rmse_valid 0.798217, mae_valid 0.591224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=allow_soft_placement,\n",
    "        log_device_placement=log_device_placement)\n",
    "    session_conf.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "\n",
    "        deep = narre(\n",
    "            review_num_u=review_num_u,\n",
    "            review_num_i=review_num_i,\n",
    "            review_len_u=review_len_u,\n",
    "            review_len_i=review_len_i,\n",
    "            user_num=user_num,\n",
    "            item_num=item_num,\n",
    "            num_classes=1,\n",
    "            user_vocab_size=len(vocabulary_user),\n",
    "            item_vocab_size=len(vocabulary_item),\n",
    "            embedding_size=embedding_dim,\n",
    "            embedding_id=embedding_id,\n",
    "            filter_sizes=list(map(int, filter_sizes.split(\",\"))),\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda,\n",
    "            attention_size=attention_size,\n",
    "            n_latent=latent_size,\n",
    "            **narre_params)\n",
    "\n",
    "        writer = tf.summary.FileWriter(logs_path, sess.graph)\n",
    "        tf.set_random_seed(random_seed)\n",
    "        print user_num\n",
    "        print item_num\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "        # optimizer = tf.train.AdagradOptimizer(learning_rate=0.01, initial_accumulator_value=1e-8).minimize(deep.loss)\n",
    "        optimizer = tf.train.AdamOptimizer(0.002, beta1=0.9, beta2=0.999, epsilon=1e-8).minimize(deep.loss, global_step=global_step)\n",
    "\n",
    "        train_op = optimizer  # .apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        if word2vec:\n",
    "            # initial matrix with random uniform\n",
    "            u = 0\n",
    "            initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_user), embedding_dim))\n",
    "            # load any vectors from the word2vec\n",
    "            print(\"Load word2vec u file {}\\n\".format(word2vec))\n",
    "            with open(word2vec, \"rb\") as f:\n",
    "                header = f.readline()\n",
    "                vocab_size, layer1_size = map(int, header.split())\n",
    "                binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "                for line in xrange(vocab_size):\n",
    "                    word = []\n",
    "                    while True:\n",
    "                        ch = f.read(1)\n",
    "                        if ch == ' ':\n",
    "                            word = ''.join(word)\n",
    "                            break\n",
    "                        if ch != '\\n':\n",
    "                            word.append(ch)\n",
    "                    idx = 0\n",
    "\n",
    "                    if word in vocabulary_user:\n",
    "                        u = u + 1\n",
    "                        idx = vocabulary_user[word]\n",
    "                        initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                    else:\n",
    "                        f.read(binary_len)\n",
    "            sess.run(deep.W1.assign(initW))\n",
    "            initW = np.random.uniform(-1.0, 1.0, (len(vocabulary_item), embedding_dim))\n",
    "            # load any vectors from the word2vec\n",
    "            print(\"Load word2vec i file {}\\n\".format(word2vec))\n",
    "\n",
    "            item = 0\n",
    "            with open(word2vec, \"rb\") as f:\n",
    "                header = f.readline()\n",
    "                vocab_size, layer1_size = map(int, header.split())\n",
    "                binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "                for line in xrange(vocab_size):\n",
    "                    word = []\n",
    "                    while True:\n",
    "                        ch = f.read(1)\n",
    "                        if ch == ' ':\n",
    "                            word = ''.join(word)\n",
    "                            break\n",
    "                        if ch != '\\n':\n",
    "                            word.append(ch)\n",
    "                    idx = 0\n",
    "                    if word in vocabulary_item:\n",
    "                        item = item + 1\n",
    "                        idx = vocabulary_item[word]\n",
    "                        initW[idx] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "                    else:\n",
    "                        f.read(binary_len)\n",
    "\n",
    "            sess.run(deep.W2.assign(initW))\n",
    "            print item\n",
    "\n",
    "        best_mae = 5\n",
    "        best_rmse = 5\n",
    "        train_mae = 0\n",
    "        train_rmse = 0\n",
    "        sum_tloss = 0\n",
    "        early_stop = 1\n",
    "        stopping_step = 0\n",
    "        should_stop = False\n",
    "\n",
    "        pkl_file = open(train_data, 'rb')\n",
    "\n",
    "        train_data = pickle.load(pkl_file)\n",
    "\n",
    "        train_data = np.array(train_data)\n",
    "        pkl_file.close()\n",
    "\n",
    "        pkl_file = open(valid_data, 'rb')\n",
    "\n",
    "        test_data = pickle.load(pkl_file)\n",
    "        test_data = np.array(test_data)\n",
    "        pkl_file.close()\n",
    "\n",
    "        data_size_train = len(train_data)\n",
    "        data_size_test = len(test_data)\n",
    "        batch_size = batch_size\n",
    "        ll = int(len(train_data) / batch_size)\n",
    "        print 'Start training'\n",
    "        start = time.time()\n",
    "        for epoch in range(num_epochs):\n",
    "            if not should_stop:\n",
    "                # Shuffle the data at each epoch\n",
    "                shuffle_indices = np.random.permutation(np.arange(data_size_train))\n",
    "                shuffled_data = train_data[shuffle_indices]\n",
    "                for batch_num in range(ll):\n",
    "\n",
    "                    start_index = batch_num * batch_size\n",
    "                    end_index = min((batch_num + 1) * batch_size, data_size_train)\n",
    "                    data_train = shuffled_data[start_index:end_index]\n",
    "\n",
    "                    uid, iid, reuid, reiid, y_batch = zip(*data_train)\n",
    "                    u_batch = []\n",
    "                    i_batch = []\n",
    "                    for i in range(len(uid)):\n",
    "                        u_batch.append(u_text[uid[i][0]])\n",
    "                        i_batch.append(i_text[iid[i][0]])\n",
    "                    u_batch = np.array(u_batch)\n",
    "                    i_batch = np.array(i_batch)\n",
    "\n",
    "                    t_loss, t_rmse, t_mae, u_a, i_a = train_step(sess, deep, u_batch, i_batch, uid, iid, reuid, reiid, y_batch,batch_num, dropout_keep_prob, train_op, global_step)\n",
    "                    current_step = tf.train.global_step(sess, global_step)\n",
    "                    sum_tloss += t_loss\n",
    "                    train_rmse += t_rmse\n",
    "                    train_mae += t_mae\n",
    "                    if epoch == 0 and batch_num == 0:\n",
    "                        print ('Finished first batch')\n",
    "                    if batch_num % 500 == 0 and batch_num > 1:\n",
    "                        print(\"\\nEvaluation:\")\n",
    "                        print batch_num\n",
    "\n",
    "                        loss_s = 0\n",
    "                        accuracy_s = 0\n",
    "                        mae_s = 0\n",
    "\n",
    "                        ll_test = int(len(test_data) / batch_size) + 1\n",
    "                        for batch_num in range(ll_test):\n",
    "                            start_index = batch_num * batch_size\n",
    "                            end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                            data_test = test_data[start_index:end_index]\n",
    "\n",
    "                            userid_valid, itemid_valid, reuid, reiid, y_valid = zip(*data_test)\n",
    "                            u_valid = []\n",
    "                            i_valid = []\n",
    "                            for i in range(len(userid_valid)):\n",
    "                                u_valid.append(u_text[userid_valid[i][0]])\n",
    "                                i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                            u_valid = np.array(u_valid)\n",
    "                            i_valid = np.array(i_valid)\n",
    "\n",
    "                            loss, accuracy, mae, _ = dev_step(sess, deep, u_valid, i_valid, userid_valid, itemid_valid, reuid, reiid,\n",
    "                                                           y_valid, global_step)\n",
    "                            loss_s = loss_s + len(u_valid) * loss\n",
    "                            accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                            mae_s = mae_s + len(u_valid) * mae\n",
    "\n",
    "                        print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                                         np.sqrt(\n",
    "                                                                                             accuracy_s / test_length),\n",
    "                                                                                         mae_s / test_length))\n",
    "\n",
    "                        rmse = np.sqrt(accuracy_s / test_length)\n",
    "                        mae = mae_s / test_length\n",
    "\n",
    "                        print(\"\")\n",
    "                        write_summary(loss_s / test_length, \"validation/loss\", writer, current_step)\n",
    "                        write_summary(rmse, \"validation/rmse\", writer, current_step)\n",
    "                        write_summary(mae, \"validation/mae\", writer, current_step)\n",
    "\n",
    "                print str(epoch) + ':\\n'\n",
    "                print(\"\\nEvaluation:\")\n",
    "                print \"train:rmse,mae:\", train_rmse / ll, train_mae / ll\n",
    "                u_a = np.reshape(u_a[0], (1, -1))\n",
    "                i_a = np.reshape(i_a[0], (1, -1))\n",
    "\n",
    "\n",
    "                write_summary(sum_tloss / ll, \"train/loss\", writer, current_step)\n",
    "                write_summary(train_rmse / ll, \"train/rmse\", writer, current_step)\n",
    "                write_summary(train_mae / ll, \"train/mae\", writer, current_step)\n",
    "\n",
    "                print u_a\n",
    "                print i_a\n",
    "                train_rmse = 0\n",
    "                train_mae = 0\n",
    "                sum_tloss = 0\n",
    "\n",
    "\n",
    "\n",
    "            loss_s = 0\n",
    "            accuracy_s = 0\n",
    "            mae_s = 0  \n",
    "\n",
    "            ll_test = int(len(test_data) / batch_size) + 1\n",
    "            for batch_num in range(ll_test):\n",
    "                start_index = batch_num * batch_size\n",
    "                end_index = min((batch_num + 1) * batch_size, data_size_test)\n",
    "                data_test = test_data[start_index:end_index]\n",
    "\n",
    "                userid_valid, itemid_valid, reuid, reiid, y_valid = zip(*data_test)\n",
    "                u_valid = []\n",
    "                i_valid = []\n",
    "                for i in range(len(userid_valid)):\n",
    "                    u_valid.append(u_text[userid_valid[i][0]])\n",
    "                    i_valid.append(i_text[itemid_valid[i][0]])\n",
    "                u_valid = np.array(u_valid)\n",
    "                i_valid = np.array(i_valid)\n",
    "\n",
    "                loss, accuracy, mae, preds = dev_step(sess, deep, u_valid, i_valid, userid_valid, itemid_valid, reuid, reiid, y_valid, global_step)\n",
    "                loss_s = loss_s + len(u_valid) * loss\n",
    "                accuracy_s = accuracy_s + len(u_valid) * np.square(accuracy)\n",
    "                mae_s = mae_s + len(u_valid) * mae\n",
    "            print (\"loss_valid {:g}, rmse_valid {:g}, mae_valid {:g}\".format(loss_s / test_length,\n",
    "                                                                             np.sqrt(accuracy_s / test_length),\n",
    "                                                                             mae_s / test_length))\n",
    "            rmse = np.sqrt(accuracy_s / test_length)\n",
    "            mae = mae_s / test_length\n",
    "\n",
    "            if best_rmse > rmse:\n",
    "                stopping_step = 0\n",
    "                best_rmse = rmse\n",
    "                best_preds = preds\n",
    "\n",
    "            else:\n",
    "                stopping_step += 1\n",
    "                if stopping_step >= early_stop:\n",
    "                    should_stop = True\n",
    "                    print (\"Early stopping is trigger at epoch: {} loss:{}\".format(epoch,loss_s / test_length))\n",
    "\n",
    "            if best_mae > mae:\n",
    "                best_mae = mae\n",
    "        print(\"\")\n",
    "        end = time.time()\n",
    "        training_time = end-start\n",
    "        filename = narre_version + '_' + dataset + '_' + datetime.datetime.now().strftime('run-%Y-%m-%d-%H-%M') + '.csv'\n",
    "        results = {'model':narre_version, 'data':dataset, 'training time':training_time, 'rmse':best_rmse, 'mae':best_mae, 'preds':best_preds,\n",
    "                  'embedding dim': embedding_dim, 'filter sizes': filter_sizes, 'num of filters': num_filters,\n",
    "                   'dropout prob': dropout_keep_prob,'l2_reg': l2_reg_lambda, 'embedding_id': embedding_id,\n",
    "                   'latent size': latent_size, 'attention_size': attention_size, 'batch size': batch_size, 'epochs': num_epochs}\n",
    "\n",
    "\n",
    "        with open('./output/' + filename, 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for key, value in results.items():\n",
    "                writer.writerow([key, value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/nirku/.conda/envs/rs_project/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n",
      "    \"__main__\", fname, loader, pkg_name)\r\n",
      "  File \"/home/nirku/.conda/envs/rs_project/lib/python2.7/runpy.py\", line 72, in _run_code\r\n",
      "    exec code in run_globals\r\n",
      "  File \"/home/nirku/.conda/envs/rs_project/lib/python2.7/site-packages/tensorboard/main.py\", line 40, in <module>\r\n",
      "    from tensorboard import default\r\n",
      "  File \"/home/nirku/.conda/envs/rs_project/lib/python2.7/site-packages/tensorboard/default.py\", line 48, in <module>\r\n",
      "    from tensorboard.plugins.interactive_inference import (\r\n",
      "  File \"/home/nirku/.conda/envs/rs_project/lib/python2.7/site-packages/tensorboard/plugins/interactive_inference/interactive_inference_plugin_loader.py\", line 15, in <module>\r\n",
      "    \"\"\"Wrapper around plugin to conditionally enable it.\"\"\"\r\n",
      "KeyboardInterrupt\r\n"
     ]
    }
   ],
   "source": [
    "!python -m tensorboard.main --logdir=./logs/ --host=132.72.46.8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rs_project",
   "language": "python",
   "name": "rs_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "707px",
    "left": "1550px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
